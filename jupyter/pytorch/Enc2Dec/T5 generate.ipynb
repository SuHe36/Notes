{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch1.4/lib/python3.7/site-packages/pandas/core/frame.py:4169: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "filename = \"./data/quora_paraphrase/quora_duplicate_questions.tsv\"\n",
    "import pandas as pd\n",
    "question_pairs = pd.read_csv(filename, sep = '\\t')\n",
    "question_pairs.drop(['qid1','qid2'], axis=1, inplace=True)\n",
    "\n",
    "question_pairs_correct_paraphrased = question_pairs[question_pairs['is_duplicate']==1]\n",
    "question_pairs_correct_paraphrased.drop(['id','is_duplicate'], axis=1, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(question_pairs_correct_paraphrased, test_size=0.1)\n",
    "\n",
    "train.to_csv('./data/quora_paraphrase/quora_train.csv', index = False)\n",
    "test.to_csv('./data/quora_paraphrase/quora_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hesu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AdamW, \n",
    "                          T5ForConditionalGeneration, \n",
    "                          T5Tokenizer,\n",
    "                          get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "        \n",
    "    def is_logger(self):\n",
    "        return self.trainer.proc_rank <= 0\n",
    "    \n",
    "    def forward(self,\n",
    "               input_ids, \n",
    "               attention_mask=None,\n",
    "               decoder_input_ids=None,\n",
    "               decoder_attention_mask=None,\n",
    "               lm_labels=None):\n",
    "        \n",
    "        return self.model(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          decoder_input_ids=decoder_input_ids,\n",
    "                          decoder_attention_mask=decoder_attention_mask,\n",
    "                          lm_labels=lm_labels,)\n",
    "        \n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:,:] == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch[\"target_mask\"])\n",
    "        \n",
    "        \n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        \n",
    "        tensorboard_logs = {\"train_loss\":loss}\n",
    "        return {\"loss\":loss, \"log\": tensorboard_logs}\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\"avg_train_loss\":avg_train_loss, \"log\":tensorboard_logs, \"progress_bar\":tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\":loss}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\":avg_loss, \"log\":tensorboard_logs, \"progress_bar\":tensorboard_logs}\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "        \n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\":[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\":self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\":[p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\":0.0,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "        \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        if self.trainer.use_tpu:\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad() # 将每个batch的梯度初始化为0\n",
    "        self.lr_scheduler.step()\n",
    "        \n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\":\"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        \n",
    "        return tqdm_dict\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, \n",
    "                                    type_path=\"./data/quora_paraphrase/quora_train\",\n",
    "                                    args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=self.hparams.train_batch_size,\n",
    "                                drop_last=True,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4)\n",
    "        \n",
    "        t_total = ((len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "                  // self.hparams.gradient_accumulation_steps\n",
    "                  * float(self.hparams.num_train_epochs))\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "                    self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        \n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"./data/quora_paraphrase/quora_test\",args=self.hparams)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"**** Validation results ****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            \n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {} \\n\".format(key, str(metrics[key])))\n",
    "    \n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"**** Test results ****\")\n",
    "        \n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            \n",
    "            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                 question1  \\\n",
      "0              Why is everyone on Quora obsessed with IQ?   \n",
      "1       Can people still see questions that are marked...   \n",
      "2       How do you reinstall the Apple app store on a ...   \n",
      "3                           Why is mathematics important?   \n",
      "4       Who makes the final decision on which question...   \n",
      "...                                                   ...   \n",
      "134331  What are some of the good books to read if one...   \n",
      "134332  Why do people waste time waiting for answers o...   \n",
      "134333                How did the Big Bang \"create\" time?   \n",
      "134334  What should I do to improve my basic in chemis...   \n",
      "134335  Is it possible to have civilization without laws?   \n",
      "\n",
      "                                                question2  \n",
      "0       What is with the obsession of people on Quora ...  \n",
      "1       What happens to questions marked as needing im...  \n",
      "2       How can I reinstall the app store on my iPad o...  \n",
      "3                        Why is mathematics so important?  \n",
      "4       How does Quora's algorithm work in selecting w...  \n",
      "...                                                   ...  \n",
      "134331  What are the the best books to read on human p...  \n",
      "134332  Why do people bother to ask questions on Quora...  \n",
      "134333  Was there existence of time before big bang? I...  \n",
      "134334          How can I improve my basics in chemistry?  \n",
      "134335             Can civilization survive without laws?  \n",
      "\n",
      "[134336 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "args_dict = dict(\n",
    "    data_dit = \"\",\n",
    "    output_dir=\"./model/\",\n",
    "    model_name_or_path=\"./model/t5_paraphraser\",\n",
    "    tokenizer_name_or_path=\"./model/t5_paraphraser\",\n",
    "    max_seq_length=512,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=6,\n",
    "    eval_batch_size=6,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False,\n",
    "    opt_level=\"01\",\n",
    "    max_grad_norm=1.0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "train_path = \"./data/quora_paraphrase/quora_train.csv\"\n",
    "val_path = \"./data/quora_paraphrase/quora_test.csv\"\n",
    "train = pd.read_csv(train_path)\n",
    "print(train.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:Model name './model/t5_paraphraser' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming './model/t5_paraphraser' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils:Didn't find file ./model/t5_paraphraser/added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file ./model/t5_paraphraser/special_tokens_map.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file ./model/t5_paraphraser/tokenizer_config.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:loading file ./model/t5_paraphraser/spiece.model\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('./model/t5_paraphraser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, type_path, max_len=256):\n",
    "        self.path = os.path.join(data_dir, type_path +\".csv\")\n",
    "        self.source_column = \"question1\"\n",
    "        self.target_column = \"question2\"\n",
    "        self.data = pd.read_csv(self.path)\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._build()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "        \n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()\n",
    "        \n",
    "        return {\"source_ids\":source_ids, \"source_mask\":src_mask, \n",
    "                \"target_ids\":target_ids, \"target_mask\":target_mask}\n",
    "    \n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            input_, target = self.data.loc[idx, self.source_column],  self.data.loc[idx, self.target_column]\n",
    "            \n",
    "            input_ = \"paraphrase: \" + input_ + \"</s>\"\n",
    "            target = target + \"</s>\"\n",
    "            \n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                    [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                    [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ParaphraseDataset(tokenizer,'./data/quora_paraphrase', 'quora_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:  134336\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset: \", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paraphrase: How do you know if you're unconditionally in love with someone?\n",
      "How can you know if you're in love or just attracted to someone?\n"
     ]
    }
   ],
   "source": [
    "data = dataset[61]\n",
    "print(tokenizer.decode(data[\"source_ids\"]))\n",
    "print(tokenizer.decode(data[\"target_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./model/t5_paraphrase'):\n",
    "    os.makedirs(\"./model/t5_paraphrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dit': '', 'output_dir': './model/t5_paraphrase', 'model_name_or_path': './model/t5_paraphraser', 'tokenizer_name_or_path': './model/t5_paraphraser', 'max_seq_length': 256, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 6, 'eval_batch_size': 6, 'num_train_epochs': 2, 'gradient_accumulation_steps': 16, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': '01', 'max_grad_norm': 1.0, 'seed': 42, 'data_dir': './data/quora_paraphrase'}\n"
     ]
    }
   ],
   "source": [
    "args_dict.update({'data_dir':'./data/quora_paraphrase', \n",
    "                  'output_dir':'./model/t5_paraphrase', \n",
    "                  'num_train_epochs':2, \n",
    "                  'max_seq_length':256})\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(filepath=args.output_dir,\n",
    "                                                  prefix=\"checkpoint\",\n",
    "                                                  monitor=\"val_loss\",\n",
    "                                                  mode=\"min\",\n",
    "                                                  save_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    early_stop_callback=False,\n",
    "    precision=16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path,args):\n",
    "    return ParaphraseDataset(tokenizer=tokenizer, \n",
    "                             data_dir=args.data_dir,\n",
    "                             type_path=type_path, \n",
    "                             max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file ./model/t5_paraphraser/config.json\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file ./model/t5_paraphraser/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:Model name './model/t5_paraphraser' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming './model/t5_paraphraser' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils:Didn't find file ./model/t5_paraphraser/added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file ./model/t5_paraphraser/special_tokens_map.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file ./model/t5_paraphraser/tokenizer_config.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:loading file ./model/t5_paraphraser/spiece.model\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize model\")\n",
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "\n                You requested GPUs: [0]\n                But your machine only has: []\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-70e3c72b7c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/torch1.4/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, early_stop_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, num_tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_pct, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, train_percent_check, val_percent_check, test_percent_check, val_check_interval, log_save_interval, row_log_interval, add_row_log_interval, distributed_backend, precision, print_nan_grads, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, progress_bar_callback, amp_level, default_save_path, gradient_clip, nb_gpu_nodes, max_nb_epochs, min_nb_epochs, use_amp, show_progress_bar, nb_sanity_val_steps, terminate_on_nan, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_root_gpu_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch1.4/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36mparse_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_parse_gpu_string_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_parse_gpu_input_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch1.4/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msanitize_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mYou\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0mBut\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0monly\u001b[0m \u001b[0mhas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mall_available_gpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \"\"\")\n\u001b[0m\u001b[1;32m    679\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: \n                You requested GPUs: [0]\n                But your machine only has: []\n            "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
