{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>5692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7022747</td>\n",
       "      <td>5558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7023662</td>\n",
       "      <td>4626</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7025965</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7029547</td>\n",
       "      <td>4449</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>579346</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7039142</td>\n",
       "      <td>5458</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>581706</td>\n",
       "      <td>4565</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>7042700</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>20042606</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp   user_id  content_id  content_type_id  answered_correctly  \\\n",
       "0          0       115        5692                0                   1   \n",
       "1          0   7022747        5558                0                   0   \n",
       "2          0   7023662        4626                0                   1   \n",
       "3          0   7025965        7900                0                   1   \n",
       "4          0   7029547        4449                0                   1   \n",
       "5          0    579346        7900                0                   1   \n",
       "6          0   7039142        5458                0                   1   \n",
       "7          0    581706        4565                0                   1   \n",
       "8          0   7042700        7900                0                   1   \n",
       "9          0  20042606        7900                0                   0   \n",
       "\n",
       "   prior_question_elapsed_time  prior_question_had_explanation  \n",
       "0                          NaN                            <NA>  \n",
       "1                          NaN                            <NA>  \n",
       "2                          NaN                            <NA>  \n",
       "3                          NaN                            <NA>  \n",
       "4                          NaN                            <NA>  \n",
       "5                          NaN                            <NA>  \n",
       "6                          NaN                            <NA>  \n",
       "7                          NaN                            <NA>  \n",
       "8                          NaN                            <NA>  \n",
       "9                          NaN                            <NA>  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/hesu/Documents/KT/riiid/train_1M.csv',\n",
    "                   usecols = [1,2,3,4,7,8,9],\n",
    "                   dtype={'timestamp':'int64',\n",
    "                         'used_id':'int32',\n",
    "                         'content_id':'int16',\n",
    "                         'content_type_id':'int8',\n",
    "                         'answered_correctly':'int8',\n",
    "                         'prior_question_elapsed_time':'float32',\n",
    "                         'prior_question_had_explanation':'boolean'})\n",
    "\n",
    "train = train[train.content_type_id == False]\n",
    "\n",
    "train = train.sort_values(['timestamp'],ascending=True).reset_index(drop=True)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51 131 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>131 36 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 101 162 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 149 162 29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>131 5 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>131 149 162 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10 94 162 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61 110 162 29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>131 13 162 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10 164 81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id  bundle_id  correct_answer  part            tags\n",
       "0            0          0               0     1   51 131 162 38\n",
       "1            1          1               1     1       131 36 81\n",
       "2            2          2               0     1  131 101 162 92\n",
       "3            3          3               0     1  131 149 162 29\n",
       "4            4          4               3     1    131 5 162 38\n",
       "5            5          5               2     1  131 149 162 81\n",
       "6            6          6               2     1    10 94 162 92\n",
       "7            7          7               0     1   61 110 162 29\n",
       "8            8          8               3     1   131 13 162 92\n",
       "9            9          9               3     1       10 164 81"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = pd.read_csv('/Users/hesu/Documents/KT/riiid/questions.csv')\n",
    "question.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5692</td>\n",
       "      <td>5692</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7022747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5558</td>\n",
       "      <td>5558</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7023662</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4626</td>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7025965</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7029547</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4449</td>\n",
       "      <td>4449</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>579346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7039142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5458</td>\n",
       "      <td>5458</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>581706</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4565</td>\n",
       "      <td>4565</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>7042700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>20042606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp   user_id  content_type_id  answered_correctly  \\\n",
       "0          0       115                0                   1   \n",
       "1          0   7022747                0                   0   \n",
       "2          0   7023662                0                   1   \n",
       "3          0   7025965                0                   1   \n",
       "4          0   7029547                0                   1   \n",
       "5          0    579346                0                   1   \n",
       "6          0   7039142                0                   1   \n",
       "7          0    581706                0                   1   \n",
       "8          0   7042700                0                   1   \n",
       "9          0  20042606                0                   0   \n",
       "\n",
       "   prior_question_elapsed_time  prior_question_had_explanation  question_id  \\\n",
       "0                          NaN                            <NA>         5692   \n",
       "1                          NaN                            <NA>         5558   \n",
       "2                          NaN                            <NA>         4626   \n",
       "3                          NaN                            <NA>         7900   \n",
       "4                          NaN                            <NA>         4449   \n",
       "5                          NaN                            <NA>         7900   \n",
       "6                          NaN                            <NA>         5458   \n",
       "7                          NaN                            <NA>         4565   \n",
       "8                          NaN                            <NA>         7900   \n",
       "9                          NaN                            <NA>         7900   \n",
       "\n",
       "   bundle_id  correct_answer  part       tags  \n",
       "0       5692               3     5        151  \n",
       "1       5558               1     5        125  \n",
       "2       4626               2     5         79  \n",
       "3       7900               0     1  131 93 81  \n",
       "4       4449               0     5        156  \n",
       "5       7900               0     1  131 93 81  \n",
       "6       5458               1     5        125  \n",
       "7       4565               0     5          8  \n",
       "8       7900               0     1  131 93 81  \n",
       "9       7900               0     1  131 93 81  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ques = pd.merge(train, question, left_on='content_id',right_on='question_id', how='left')\n",
    "train_ques.drop('content_id',axis=1,inplace=True)\n",
    "train_ques.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>980082</th>\n",
       "      <td>76809860397</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3016</td>\n",
       "      <td>3015</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>157 171 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980083</th>\n",
       "      <td>76809860397</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3015</td>\n",
       "      <td>3015</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>136 171 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980084</th>\n",
       "      <td>76810038254</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3068</td>\n",
       "      <td>3066</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>113 12 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980085</th>\n",
       "      <td>76810038254</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3067</td>\n",
       "      <td>3066</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>74 12 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980086</th>\n",
       "      <td>76810038254</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3066</td>\n",
       "      <td>3066</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>106 12 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980087</th>\n",
       "      <td>78091996556</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>7398</td>\n",
       "      <td>7396</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>97 160 16 35 122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980088</th>\n",
       "      <td>78091996556</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>7399</td>\n",
       "      <td>7396</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>97 160 16 35 122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980089</th>\n",
       "      <td>78091996556</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>7397</td>\n",
       "      <td>7396</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18 160 16 35 122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980090</th>\n",
       "      <td>78091996556</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>7396</td>\n",
       "      <td>7396</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>39 160 16 35 122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980091</th>\n",
       "      <td>78091996556</td>\n",
       "      <td>4508124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28666.0</td>\n",
       "      <td>True</td>\n",
       "      <td>7400</td>\n",
       "      <td>7396</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>145 160 16 35 122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp  user_id  content_type_id  answered_correctly  \\\n",
       "980082  76809860397  4508124                0                   1   \n",
       "980083  76809860397  4508124                0                   1   \n",
       "980084  76810038254  4508124                0                   1   \n",
       "980085  76810038254  4508124                0                   0   \n",
       "980086  76810038254  4508124                0                   1   \n",
       "980087  78091996556  4508124                0                   1   \n",
       "980088  78091996556  4508124                0                   0   \n",
       "980089  78091996556  4508124                0                   1   \n",
       "980090  78091996556  4508124                0                   0   \n",
       "980091  78091996556  4508124                0                   1   \n",
       "\n",
       "        prior_question_elapsed_time  prior_question_had_explanation  \\\n",
       "980082                      35000.0                            True   \n",
       "980083                      35000.0                            True   \n",
       "980084                      32666.0                            True   \n",
       "980085                      32666.0                            True   \n",
       "980086                      32666.0                            True   \n",
       "980087                      28666.0                            True   \n",
       "980088                      28666.0                            True   \n",
       "980089                      28666.0                            True   \n",
       "980090                      28666.0                            True   \n",
       "980091                      28666.0                            True   \n",
       "\n",
       "        question_id  bundle_id  correct_answer  part               tags  \n",
       "980082         3016       3015               0     4         157 171 92  \n",
       "980083         3015       3015               2     4         136 171 92  \n",
       "980084         3068       3066               3     4      113 12 162 38  \n",
       "980085         3067       3066               0     4       74 12 162 38  \n",
       "980086         3066       3066               1     4      106 12 162 38  \n",
       "980087         7398       7396               2     7   97 160 16 35 122  \n",
       "980088         7399       7396               0     7   97 160 16 35 122  \n",
       "980089         7397       7396               1     7   18 160 16 35 122  \n",
       "980090         7396       7396               1     7   39 160 16 35 122  \n",
       "980091         7400       7396               1     7  145 160 16 35 122  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ques.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_mean = train_ques.prior_question_elapsed_time.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ques['prior_question_elapsed_time'].fillna(elapsed_mean, inplace=True)\n",
    "train_ques['part'].fillna(4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000.0     50744\n",
       "16000.0     46949\n",
       "18000.0     46550\n",
       "19000.0     39580\n",
       "15000.0     35889\n",
       "            ...  \n",
       "135200.0        1\n",
       "121750.0        1\n",
       "119250.0        1\n",
       "150200.0        1\n",
       "99333.0         1\n",
       "Name: prior_question_elapsed_time, Length: 1660, dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ques.loc[:,'prior_question_elapsed_time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    403239\n",
       "2    190731\n",
       "6    108567\n",
       "3     82175\n",
       "4     75997\n",
       "1     69411\n",
       "7     49972\n",
       "Name: part, dtype: int64"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ques.loc[:,'part'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972-06-23 04:13:16\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "def convert_time_to_yearMonthDay(timeStamp):\n",
    "    timeStamp = timeStamp /1000.0\n",
    "    timearr = time.localtime(timeStamp)\n",
    "    otherStyleTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", timearr)\n",
    "    print(otherStyleTime)\n",
    "\n",
    "convert_time_to_yearMonthDay(78091996556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elapsed_time(ela):\n",
    "    ela = ela // 1000\n",
    "    if ela > 300:\n",
    "        return 300\n",
    "    else:\n",
    "        return ela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ques['prior_question_elapsed_time'] = train_ques['prior_question_elapsed_time'].apply(lambda x: get_elapsed_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5692</td>\n",
       "      <td>5692</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7022747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5558</td>\n",
       "      <td>5558</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7023662</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4626</td>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7025965</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7029547</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4449</td>\n",
       "      <td>4449</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>579346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7039142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5458</td>\n",
       "      <td>5458</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>581706</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4565</td>\n",
       "      <td>4565</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>7042700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>20042606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 93 81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp   user_id  content_type_id  answered_correctly  \\\n",
       "0          0       115                0                   1   \n",
       "1          0   7022747                0                   0   \n",
       "2          0   7023662                0                   1   \n",
       "3          0   7025965                0                   1   \n",
       "4          0   7029547                0                   1   \n",
       "5          0    579346                0                   1   \n",
       "6          0   7039142                0                   1   \n",
       "7          0    581706                0                   1   \n",
       "8          0   7042700                0                   1   \n",
       "9          0  20042606                0                   0   \n",
       "\n",
       "   prior_question_elapsed_time  prior_question_had_explanation  question_id  \\\n",
       "0                         25.0                            <NA>         5692   \n",
       "1                         25.0                            <NA>         5558   \n",
       "2                         25.0                            <NA>         4626   \n",
       "3                         25.0                            <NA>         7900   \n",
       "4                         25.0                            <NA>         4449   \n",
       "5                         25.0                            <NA>         7900   \n",
       "6                         25.0                            <NA>         5458   \n",
       "7                         25.0                            <NA>         4565   \n",
       "8                         25.0                            <NA>         7900   \n",
       "9                         25.0                            <NA>         7900   \n",
       "\n",
       "   bundle_id  correct_answer  part       tags  \n",
       "0       5692               3     5        151  \n",
       "1       5558               1     5        125  \n",
       "2       4626               2     5         79  \n",
       "3       7900               0     1  131 93 81  \n",
       "4       4449               0     5        156  \n",
       "5       7900               0     1  131 93 81  \n",
       "6       5458               1     5        125  \n",
       "7       4565               0     5          8  \n",
       "8       7900               0     1  131 93 81  \n",
       "9       7900               0     1  131 93 81  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ques.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ques['timestamp'] = train_ques['timestamp'].astype(str)\n",
    "train_ques['question_id'] = train_ques['question_id'].astype(str)\n",
    "train_ques['part'] = train_ques['part'].astype(str)\n",
    "train_ques['prior_question_elapsed_time'] = train_ques['prior_question_elapsed_time'].astype(str)\n",
    "train_ques['answered_correctly'] = train_ques['answered_correctly'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = train_ques.groupby('user_id').agg({\"question_id\": ','.join, \n",
    "                                                \"answered_correctly\":','.join,\n",
    "                                                \"timestamp\":','.join,\n",
    "                                                \"part\":','.join,\n",
    "                                                \"prior_question_elapsed_time\":','.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>part</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5692,5716,128,7860,7922,156,51,50,7896,7863,15...</td>\n",
       "      <td>1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,...</td>\n",
       "      <td>0,56943,118363,131167,137965,157063,176092,194...</td>\n",
       "      <td>5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,37.0,55.0,19.0,11.0,5.0,17.0,17.0,16.0,16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>7900,7876,175,1278,2064,2065,2063,3363,3364,33...</td>\n",
       "      <td>1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,...</td>\n",
       "      <td>0,32683,62000,83632,189483,189483,189483,25879...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,26.0,29.0,26.0,18.0,18.0,18.0,33.0,33.0,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>5273,758,5976,236,404,382,405,873,531,775,294,...</td>\n",
       "      <td>0,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,1</td>\n",
       "      <td>0,21592,49069,72254,91945,111621,134341,234605...</td>\n",
       "      <td>5,2,5,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2</td>\n",
       "      <td>25.0,28.0,17.0,24.0,20.0,16.0,16.0,19.0,18.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>5000,3944,217,5844,5965,4990,5235,6050,5721,55...</td>\n",
       "      <td>1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,1,1,0,0,0,...</td>\n",
       "      <td>0,39828,132189,153727,169080,178049,274437,348...</td>\n",
       "      <td>5,5,2,5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,24.0,35.0,88.0,18.0,12.0,5.0,92.0,70.0,14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>3915,4750,6456,3968,6104,5738,6435,5498,6102,4...</td>\n",
       "      <td>1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,...</td>\n",
       "      <td>0,38769,72859,116541,155537,189115,221413,2399...</td>\n",
       "      <td>5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,5,5,5,5,5,5,5,...</td>\n",
       "      <td>25.0,16.0,33.0,30.0,40.0,35.0,30.0,29.0,15.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>3901,6671,4963,6143,8279,3964,4002,754,1110,77...</td>\n",
       "      <td>1,1,1,0,1,0,0,0,1,0,0,1,1,1,0,1,1</td>\n",
       "      <td>0,17833,45872,74561,121601,141679,183773,11482...</td>\n",
       "      <td>5,5,5,5,5,5,5,2,2,2,2,2,2,2,2,2,2</td>\n",
       "      <td>25.0,13.0,15.0,24.0,25.0,44.0,17.0,39.0,16.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12741</th>\n",
       "      <td>5145,9691,9697,5202,4787,5695,7858,5653,5889,4...</td>\n",
       "      <td>0,1,0,1,1,0,1,0,1,0,0,0,1,1,1,0,1,0,0,1,1,0,0,...</td>\n",
       "      <td>0,22273,54323,92046,109716,132679,158477,18403...</td>\n",
       "      <td>5,5,5,5,5,5,1,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,...</td>\n",
       "      <td>25.0,13.0,18.0,29.0,35.0,15.0,21.0,23.0,23.0,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13134</th>\n",
       "      <td>3926,564,3865,4231,3684,3988,3968,5219,4447,61...</td>\n",
       "      <td>1,0,0,1,1,0,0,1,1,0,1,1,1,0,1,1,0,1,0,1,0,1,1,...</td>\n",
       "      <td>0,23840,46834,64749,113000,183369,218217,29783...</td>\n",
       "      <td>5,2,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,2,...</td>\n",
       "      <td>25.0,22.0,18.0,19.0,13.0,43.0,65.0,31.0,5.0,17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24418</th>\n",
       "      <td>7900,7876,175,1278,2063,2065,2064,3363,3364,33...</td>\n",
       "      <td>0,0,1,1,0,0,0,0,1,1,0,0,1,0,0,0,1,0,1,0,0,1,1,...</td>\n",
       "      <td>0,24224,51020,70540,88142,88142,88142,100241,1...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,30.0,20.0,24.0,17.0,17.0,17.0,4.0,4.0,4.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24600</th>\n",
       "      <td>7900,7876,175,1278,2063,2065,2064,3365,3363,33...</td>\n",
       "      <td>1,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,...</td>\n",
       "      <td>0,25379,50137,70181,148601,148601,148601,21935...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,24.0,23.0,22.0,18.0,18.0,18.0,24.0,24.0,2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question_id  \\\n",
       "user_id                                                      \n",
       "115      5692,5716,128,7860,7922,156,51,50,7896,7863,15...   \n",
       "124      7900,7876,175,1278,2064,2065,2063,3363,3364,33...   \n",
       "2746     5273,758,5976,236,404,382,405,873,531,775,294,...   \n",
       "5382     5000,3944,217,5844,5965,4990,5235,6050,5721,55...   \n",
       "8623     3915,4750,6456,3968,6104,5738,6435,5498,6102,4...   \n",
       "8701     3901,6671,4963,6143,8279,3964,4002,754,1110,77...   \n",
       "12741    5145,9691,9697,5202,4787,5695,7858,5653,5889,4...   \n",
       "13134    3926,564,3865,4231,3684,3988,3968,5219,4447,61...   \n",
       "24418    7900,7876,175,1278,2063,2065,2064,3363,3364,33...   \n",
       "24600    7900,7876,175,1278,2063,2065,2064,3365,3363,33...   \n",
       "\n",
       "                                        answered_correctly  \\\n",
       "user_id                                                      \n",
       "115      1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,...   \n",
       "124      1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,...   \n",
       "2746                 0,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,1   \n",
       "5382     1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,1,1,0,0,0,...   \n",
       "8623     1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,...   \n",
       "8701                     1,1,1,0,1,0,0,0,1,0,0,1,1,1,0,1,1   \n",
       "12741    0,1,0,1,1,0,1,0,1,0,0,0,1,1,1,0,1,0,0,1,1,0,0,...   \n",
       "13134    1,0,0,1,1,0,0,1,1,0,1,1,1,0,1,1,0,1,0,1,0,1,1,...   \n",
       "24418    0,0,1,1,0,0,0,0,1,1,0,0,1,0,0,0,1,0,1,0,0,1,1,...   \n",
       "24600    1,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,...   \n",
       "\n",
       "                                                 timestamp  \\\n",
       "user_id                                                      \n",
       "115      0,56943,118363,131167,137965,157063,176092,194...   \n",
       "124      0,32683,62000,83632,189483,189483,189483,25879...   \n",
       "2746     0,21592,49069,72254,91945,111621,134341,234605...   \n",
       "5382     0,39828,132189,153727,169080,178049,274437,348...   \n",
       "8623     0,38769,72859,116541,155537,189115,221413,2399...   \n",
       "8701     0,17833,45872,74561,121601,141679,183773,11482...   \n",
       "12741    0,22273,54323,92046,109716,132679,158477,18403...   \n",
       "13134    0,23840,46834,64749,113000,183369,218217,29783...   \n",
       "24418    0,24224,51020,70540,88142,88142,88142,100241,1...   \n",
       "24600    0,25379,50137,70181,148601,148601,148601,21935...   \n",
       "\n",
       "                                                      part  \\\n",
       "user_id                                                      \n",
       "115      5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "124      1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "2746                 5,2,5,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2   \n",
       "5382     5,5,2,5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "8623     5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,5,5,5,5,5,5,5,...   \n",
       "8701                     5,5,5,5,5,5,5,2,2,2,2,2,2,2,2,2,2   \n",
       "12741    5,5,5,5,5,5,1,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,...   \n",
       "13134    5,2,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,2,...   \n",
       "24418    1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "24600    1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "\n",
       "                               prior_question_elapsed_time  \n",
       "user_id                                                     \n",
       "115      25.0,37.0,55.0,19.0,11.0,5.0,17.0,17.0,16.0,16...  \n",
       "124      25.0,26.0,29.0,26.0,18.0,18.0,18.0,33.0,33.0,3...  \n",
       "2746     25.0,28.0,17.0,24.0,20.0,16.0,16.0,19.0,18.0,1...  \n",
       "5382     25.0,24.0,35.0,88.0,18.0,12.0,5.0,92.0,70.0,14...  \n",
       "8623     25.0,16.0,33.0,30.0,40.0,35.0,30.0,29.0,15.0,1...  \n",
       "8701     25.0,13.0,15.0,24.0,25.0,44.0,17.0,39.0,16.0,1...  \n",
       "12741    25.0,13.0,18.0,29.0,35.0,15.0,21.0,23.0,23.0,3...  \n",
       "13134    25.0,22.0,18.0,19.0,13.0,43.0,65.0,31.0,5.0,17...  \n",
       "24418    25.0,30.0,20.0,24.0,17.0,17.0,17.0,4.0,4.0,4.0...  \n",
       "24600    25.0,24.0,23.0,22.0,18.0,18.0,18.0,24.0,24.0,2...  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3824, 5)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>part</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5692,5716,128,7860,7922,156,51,50,7896,7863,15...</td>\n",
       "      <td>1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,...</td>\n",
       "      <td>0,56943,118363,131167,137965,157063,176092,194...</td>\n",
       "      <td>5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,37.0,55.0,19.0,11.0,5.0,17.0,17.0,16.0,16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>7900,7876,175,1278,2064,2065,2063,3363,3364,33...</td>\n",
       "      <td>1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,...</td>\n",
       "      <td>0,32683,62000,83632,189483,189483,189483,25879...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,26.0,29.0,26.0,18.0,18.0,18.0,33.0,33.0,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>5273,758,5976,236,404,382,405,873,531,775,294,...</td>\n",
       "      <td>0,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,1</td>\n",
       "      <td>0,21592,49069,72254,91945,111621,134341,234605...</td>\n",
       "      <td>5,2,5,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2</td>\n",
       "      <td>25.0,28.0,17.0,24.0,20.0,16.0,16.0,19.0,18.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>5000,3944,217,5844,5965,4990,5235,6050,5721,55...</td>\n",
       "      <td>1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,1,1,0,0,0,...</td>\n",
       "      <td>0,39828,132189,153727,169080,178049,274437,348...</td>\n",
       "      <td>5,5,2,5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,24.0,35.0,88.0,18.0,12.0,5.0,92.0,70.0,14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>3915,4750,6456,3968,6104,5738,6435,5498,6102,4...</td>\n",
       "      <td>1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,...</td>\n",
       "      <td>0,38769,72859,116541,155537,189115,221413,2399...</td>\n",
       "      <td>5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,5,5,5,5,5,5,5,...</td>\n",
       "      <td>25.0,16.0,33.0,30.0,40.0,35.0,30.0,29.0,15.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20913319</th>\n",
       "      <td>6659,5675,3841,5299,5254,4706,5318,6051,174,78...</td>\n",
       "      <td>0,1,0,1,0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,0,1,1,...</td>\n",
       "      <td>0,13518,35768,64516,86907,111406,130852,367471...</td>\n",
       "      <td>5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,27.0,10.0,20.0,26.0,20.0,22.0,16.0,12.0,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20913864</th>\n",
       "      <td>4790,4422,9200,3644,9418,9805,10405,6659,6286,...</td>\n",
       "      <td>1,0,0,1,0,0,0,0,1,1,1,0,0,0,1,0,0,0,1,0,0</td>\n",
       "      <td>0,29051,50530,60217,79747,98008,121888,158226,...</td>\n",
       "      <td>5,5,5,5,5,5,1,5,5,2,2,5,5,5,5,5,5,5,5,5,5</td>\n",
       "      <td>25.0,9.0,21.0,18.0,6.0,16.0,15.0,21.0,33.0,40....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20938253</th>\n",
       "      <td>7900,7876,175,1278,2065,2063,2064,3365,3364,33...</td>\n",
       "      <td>0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,...</td>\n",
       "      <td>0,4124,115985,130714,149045,149045,149045,1608...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,2.0,1.0,4.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20948951</th>\n",
       "      <td>6040,6444,8933,8537,10471,9236,4707,9353,8969,...</td>\n",
       "      <td>0,1,1,0,1,0,1,0,0,1,0,1,0,0,0,0,1,0,1,1,1,0,1,...</td>\n",
       "      <td>0,24764,45950,71359,95527,120065,145390,172145...</td>\n",
       "      <td>5,5,5,5,1,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,...</td>\n",
       "      <td>25.0,9.0,18.0,18.0,23.0,21.0,22.0,23.0,24.0,24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949024</th>\n",
       "      <td>7900,7876,175,1278,2065,2064,2063,3364,3365,33...</td>\n",
       "      <td>1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,...</td>\n",
       "      <td>0,25119,50251,73366,654489,654489,654489,71879...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,21.0,23.0,23.0,22.0,22.0,22.0,62.0,62.0,6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3824 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question_id  \\\n",
       "user_id                                                       \n",
       "115       5692,5716,128,7860,7922,156,51,50,7896,7863,15...   \n",
       "124       7900,7876,175,1278,2064,2065,2063,3363,3364,33...   \n",
       "2746      5273,758,5976,236,404,382,405,873,531,775,294,...   \n",
       "5382      5000,3944,217,5844,5965,4990,5235,6050,5721,55...   \n",
       "8623      3915,4750,6456,3968,6104,5738,6435,5498,6102,4...   \n",
       "...                                                     ...   \n",
       "20913319  6659,5675,3841,5299,5254,4706,5318,6051,174,78...   \n",
       "20913864  4790,4422,9200,3644,9418,9805,10405,6659,6286,...   \n",
       "20938253  7900,7876,175,1278,2065,2063,2064,3365,3364,33...   \n",
       "20948951  6040,6444,8933,8537,10471,9236,4707,9353,8969,...   \n",
       "20949024  7900,7876,175,1278,2065,2064,2063,3364,3365,33...   \n",
       "\n",
       "                                         answered_correctly  \\\n",
       "user_id                                                       \n",
       "115       1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,...   \n",
       "124       1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,...   \n",
       "2746                  0,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,1   \n",
       "5382      1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,1,1,0,0,0,...   \n",
       "8623      1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,...   \n",
       "...                                                     ...   \n",
       "20913319  0,1,0,1,0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,0,1,1,...   \n",
       "20913864          1,0,0,1,0,0,0,0,1,1,1,0,0,0,1,0,0,0,1,0,0   \n",
       "20938253  0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,...   \n",
       "20948951  0,1,1,0,1,0,1,0,0,1,0,1,0,0,0,0,1,0,1,1,1,0,1,...   \n",
       "20949024  1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,...   \n",
       "\n",
       "                                                  timestamp  \\\n",
       "user_id                                                       \n",
       "115       0,56943,118363,131167,137965,157063,176092,194...   \n",
       "124       0,32683,62000,83632,189483,189483,189483,25879...   \n",
       "2746      0,21592,49069,72254,91945,111621,134341,234605...   \n",
       "5382      0,39828,132189,153727,169080,178049,274437,348...   \n",
       "8623      0,38769,72859,116541,155537,189115,221413,2399...   \n",
       "...                                                     ...   \n",
       "20913319  0,13518,35768,64516,86907,111406,130852,367471...   \n",
       "20913864  0,29051,50530,60217,79747,98008,121888,158226,...   \n",
       "20938253  0,4124,115985,130714,149045,149045,149045,1608...   \n",
       "20948951  0,24764,45950,71359,95527,120065,145390,172145...   \n",
       "20949024  0,25119,50251,73366,654489,654489,654489,71879...   \n",
       "\n",
       "                                                       part  \\\n",
       "user_id                                                       \n",
       "115       5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "124       1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "2746                  5,2,5,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2   \n",
       "5382      5,5,2,5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "8623      5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,5,5,5,5,5,5,5,...   \n",
       "...                                                     ...   \n",
       "20913319  5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "20913864          5,5,5,5,5,5,1,5,5,2,2,5,5,5,5,5,5,5,5,5,5   \n",
       "20938253  1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "20948951  5,5,5,5,1,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,...   \n",
       "20949024  1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "\n",
       "                                prior_question_elapsed_time  \n",
       "user_id                                                      \n",
       "115       25.0,37.0,55.0,19.0,11.0,5.0,17.0,17.0,16.0,16...  \n",
       "124       25.0,26.0,29.0,26.0,18.0,18.0,18.0,33.0,33.0,3...  \n",
       "2746      25.0,28.0,17.0,24.0,20.0,16.0,16.0,19.0,18.0,1...  \n",
       "5382      25.0,24.0,35.0,88.0,18.0,12.0,5.0,92.0,70.0,14...  \n",
       "8623      25.0,16.0,33.0,30.0,40.0,35.0,30.0,29.0,15.0,1...  \n",
       "...                                                     ...  \n",
       "20913319  25.0,27.0,10.0,20.0,26.0,20.0,22.0,16.0,12.0,2...  \n",
       "20913864  25.0,9.0,21.0,18.0,6.0,16.0,15.0,21.0,33.0,40....  \n",
       "20938253  25.0,2.0,1.0,4.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,1...  \n",
       "20948951  25.0,9.0,18.0,18.0,23.0,21.0,22.0,23.0,24.0,24...  \n",
       "20949024  25.0,21.0,23.0,23.0,22.0,22.0,22.0,62.0,62.0,6...  \n",
       "\n",
       "[3824 rows x 5 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>part</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>5692,5716,128,7860,7922,156,51,50,7896,7863,15...</td>\n",
       "      <td>1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,...</td>\n",
       "      <td>0,56943,118363,131167,137965,157063,176092,194...</td>\n",
       "      <td>5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,37.0,55.0,19.0,11.0,5.0,17.0,17.0,16.0,16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>7900,7876,175,1278,2064,2065,2063,3363,3364,33...</td>\n",
       "      <td>1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,...</td>\n",
       "      <td>0,32683,62000,83632,189483,189483,189483,25879...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,26.0,29.0,26.0,18.0,18.0,18.0,33.0,33.0,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2746</td>\n",
       "      <td>5273,758,5976,236,404,382,405,873,531,775,294,...</td>\n",
       "      <td>0,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,1</td>\n",
       "      <td>0,21592,49069,72254,91945,111621,134341,234605...</td>\n",
       "      <td>5,2,5,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2</td>\n",
       "      <td>25.0,28.0,17.0,24.0,20.0,16.0,16.0,19.0,18.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5382</td>\n",
       "      <td>5000,3944,217,5844,5965,4990,5235,6050,5721,55...</td>\n",
       "      <td>1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,1,1,0,0,0,...</td>\n",
       "      <td>0,39828,132189,153727,169080,178049,274437,348...</td>\n",
       "      <td>5,5,2,5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,24.0,35.0,88.0,18.0,12.0,5.0,92.0,70.0,14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8623</td>\n",
       "      <td>3915,4750,6456,3968,6104,5738,6435,5498,6102,4...</td>\n",
       "      <td>1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,...</td>\n",
       "      <td>0,38769,72859,116541,155537,189115,221413,2399...</td>\n",
       "      <td>5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,5,5,5,5,5,5,5,...</td>\n",
       "      <td>25.0,16.0,33.0,30.0,40.0,35.0,30.0,29.0,15.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3819</th>\n",
       "      <td>20913319</td>\n",
       "      <td>6659,5675,3841,5299,5254,4706,5318,6051,174,78...</td>\n",
       "      <td>0,1,0,1,0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,0,1,1,...</td>\n",
       "      <td>0,13518,35768,64516,86907,111406,130852,367471...</td>\n",
       "      <td>5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...</td>\n",
       "      <td>25.0,27.0,10.0,20.0,26.0,20.0,22.0,16.0,12.0,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3820</th>\n",
       "      <td>20913864</td>\n",
       "      <td>4790,4422,9200,3644,9418,9805,10405,6659,6286,...</td>\n",
       "      <td>1,0,0,1,0,0,0,0,1,1,1,0,0,0,1,0,0,0,1,0,0</td>\n",
       "      <td>0,29051,50530,60217,79747,98008,121888,158226,...</td>\n",
       "      <td>5,5,5,5,5,5,1,5,5,2,2,5,5,5,5,5,5,5,5,5,5</td>\n",
       "      <td>25.0,9.0,21.0,18.0,6.0,16.0,15.0,21.0,33.0,40....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>20938253</td>\n",
       "      <td>7900,7876,175,1278,2065,2063,2064,3365,3364,33...</td>\n",
       "      <td>0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,...</td>\n",
       "      <td>0,4124,115985,130714,149045,149045,149045,1608...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,2.0,1.0,4.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822</th>\n",
       "      <td>20948951</td>\n",
       "      <td>6040,6444,8933,8537,10471,9236,4707,9353,8969,...</td>\n",
       "      <td>0,1,1,0,1,0,1,0,0,1,0,1,0,0,0,0,1,0,1,1,1,0,1,...</td>\n",
       "      <td>0,24764,45950,71359,95527,120065,145390,172145...</td>\n",
       "      <td>5,5,5,5,1,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,...</td>\n",
       "      <td>25.0,9.0,18.0,18.0,23.0,21.0,22.0,23.0,24.0,24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3823</th>\n",
       "      <td>20949024</td>\n",
       "      <td>7900,7876,175,1278,2065,2064,2063,3364,3365,33...</td>\n",
       "      <td>1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,...</td>\n",
       "      <td>0,25119,50251,73366,654489,654489,654489,71879...</td>\n",
       "      <td>1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...</td>\n",
       "      <td>25.0,21.0,23.0,23.0,22.0,22.0,22.0,62.0,62.0,6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3824 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                                        question_id  \\\n",
       "0          115  5692,5716,128,7860,7922,156,51,50,7896,7863,15...   \n",
       "1          124  7900,7876,175,1278,2064,2065,2063,3363,3364,33...   \n",
       "2         2746  5273,758,5976,236,404,382,405,873,531,775,294,...   \n",
       "3         5382  5000,3944,217,5844,5965,4990,5235,6050,5721,55...   \n",
       "4         8623  3915,4750,6456,3968,6104,5738,6435,5498,6102,4...   \n",
       "...        ...                                                ...   \n",
       "3819  20913319  6659,5675,3841,5299,5254,4706,5318,6051,174,78...   \n",
       "3820  20913864  4790,4422,9200,3644,9418,9805,10405,6659,6286,...   \n",
       "3821  20938253  7900,7876,175,1278,2065,2063,2064,3365,3364,33...   \n",
       "3822  20948951  6040,6444,8933,8537,10471,9236,4707,9353,8969,...   \n",
       "3823  20949024  7900,7876,175,1278,2065,2064,2063,3364,3365,33...   \n",
       "\n",
       "                                     answered_correctly  \\\n",
       "0     1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,...   \n",
       "1     1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,...   \n",
       "2                 0,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,1   \n",
       "3     1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,1,1,0,0,0,...   \n",
       "4     1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,...   \n",
       "...                                                 ...   \n",
       "3819  0,1,0,1,0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,0,1,1,...   \n",
       "3820          1,0,0,1,0,0,0,0,1,1,1,0,0,0,1,0,0,0,1,0,0   \n",
       "3821  0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,...   \n",
       "3822  0,1,1,0,1,0,1,0,0,1,0,1,0,0,0,0,1,0,1,1,1,0,1,...   \n",
       "3823  1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,...   \n",
       "\n",
       "                                              timestamp  \\\n",
       "0     0,56943,118363,131167,137965,157063,176092,194...   \n",
       "1     0,32683,62000,83632,189483,189483,189483,25879...   \n",
       "2     0,21592,49069,72254,91945,111621,134341,234605...   \n",
       "3     0,39828,132189,153727,169080,178049,274437,348...   \n",
       "4     0,38769,72859,116541,155537,189115,221413,2399...   \n",
       "...                                                 ...   \n",
       "3819  0,13518,35768,64516,86907,111406,130852,367471...   \n",
       "3820  0,29051,50530,60217,79747,98008,121888,158226,...   \n",
       "3821  0,4124,115985,130714,149045,149045,149045,1608...   \n",
       "3822  0,24764,45950,71359,95527,120065,145390,172145...   \n",
       "3823  0,25119,50251,73366,654489,654489,654489,71879...   \n",
       "\n",
       "                                                   part  \\\n",
       "0     5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "1     1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "2                 5,2,5,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2   \n",
       "3     5,5,2,5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "4     5,5,5,5,5,5,5,5,5,5,5,2,2,2,2,2,5,5,5,5,5,5,5,...   \n",
       "...                                                 ...   \n",
       "3819  5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...   \n",
       "3820          5,5,5,5,5,5,1,5,5,2,2,5,5,5,5,5,5,5,5,5,5   \n",
       "3821  1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "3822  5,5,5,5,1,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,...   \n",
       "3823  1,1,1,2,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,6,...   \n",
       "\n",
       "                            prior_question_elapsed_time  \n",
       "0     25.0,37.0,55.0,19.0,11.0,5.0,17.0,17.0,16.0,16...  \n",
       "1     25.0,26.0,29.0,26.0,18.0,18.0,18.0,33.0,33.0,3...  \n",
       "2     25.0,28.0,17.0,24.0,20.0,16.0,16.0,19.0,18.0,1...  \n",
       "3     25.0,24.0,35.0,88.0,18.0,12.0,5.0,92.0,70.0,14...  \n",
       "4     25.0,16.0,33.0,30.0,40.0,35.0,30.0,29.0,15.0,1...  \n",
       "...                                                 ...  \n",
       "3819  25.0,27.0,10.0,20.0,26.0,20.0,22.0,16.0,12.0,2...  \n",
       "3820  25.0,9.0,21.0,18.0,6.0,16.0,15.0,21.0,33.0,40....  \n",
       "3821  25.0,2.0,1.0,4.0,3.0,3.0,3.0,3.0,3.0,3.0,1.0,1...  \n",
       "3822  25.0,9.0,18.0,18.0,23.0,21.0,22.0,23.0,24.0,24...  \n",
       "3823  25.0,21.0,23.0,23.0,22.0,22.0,22.0,62.0,62.0,6...  \n",
       "\n",
       "[3824 rows x 6 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_train(train_user, seq_len):\n",
    "    all_ques_seq = []\n",
    "    all_ans_seq = []\n",
    "    all_parts_seq = []\n",
    "    all_ela_seq = []\n",
    "    \n",
    "    for row in train_user.itertuples():\n",
    "        q_ids = getattr(row, 'question_id').strip().split(',')\n",
    "        ans_ids = getattr(row, 'answered_correctly').strip().split(',')\n",
    "        part_ids = getattr(row, 'part').strip().split(',')\n",
    "        ela_ids = getattr(row, 'prior_question_elapsed_time').strip().split(',')\n",
    "        \n",
    "        assert len(q_ids) == len(ans_ids) == len(part_ids) == len(ela_ids)\n",
    "        for target_index in range(0, len(q_ids)):\n",
    "            q_ids_seq = q_ids[:target_index+1]\n",
    "            ans_ids_seq = ans_ids[:target_index+1]\n",
    "            part_ids_seq = part_ids[:target_index+1]\n",
    "            ela_ids_seq = ela_ids[:target_index+1]\n",
    "            \n",
    "            length = len(q_ids_seq)\n",
    "            if length >= seq_len:\n",
    "                q_ids_seq = q_ids_seq[-seq_len:]\n",
    "                ans_ids_seq = ans_ids_seq[-seq_len:]\n",
    "                part_ids_seq = part_ids_seq[-seq_len:]\n",
    "                ela_ids_seq = ela_ids_seq[-seq_len:]  \n",
    "                \n",
    "                pad_counts = 0\n",
    "            else:\n",
    "                pad_counts = seq_len - length\n",
    "            \n",
    "            q_ids_seq = [int(float(e)) for e in q_ids_seq]\n",
    "            ans_ids_seq = [int(float(e)) for e in ans_ids_seq]\n",
    "            part_ids_seq = [int(float(e)) for e in part_ids_seq]\n",
    "            ela_ids_seq = [int(float(e)) for e in ela_ids_seq]\n",
    "            \n",
    "            q_ids_seq = [13523]*pad_counts + q_ids_seq\n",
    "            # question用13523表示padding位\n",
    "            ans_ids_seq = [2]*pad_counts + [3] + ans_ids_seq\n",
    "            # ans用2表示padding位\n",
    "            # ans因为是输入到decoder中，所以需要一个起始符号，这里选择3作为其实符号，也就是句子序列中的bos的作用\n",
    "            part_ids_seq = [8]*pad_counts + part_ids_seq\n",
    "            # part用8来表示padding位\n",
    "            ela_ids_seq = [301]*pad_counts + ela_ids_seq\n",
    "            # ela用301来表示padding位\n",
    "#             print(\"q_ids length is:{}\\n ans_ids length is:{}\\n part length is:{}\\n ela_ids length is:{}\".format(len(q_ids_seq),len(ans_ids_seq),len(part_ids_seq),len(ela_ids_seq)))\n",
    "            all_ques_seq.append(q_ids_seq)\n",
    "            all_ans_seq.append(ans_ids_seq)\n",
    "            all_parts_seq.append(part_ids_seq)\n",
    "            all_ela_seq.append(ela_ids_seq)\n",
    "    return torch.LongTensor(all_ques_seq),\\\n",
    "        torch.LongTensor(all_ans_seq),\\\n",
    "        torch.LongTensor(all_parts_seq),\\\n",
    "        torch.LongTensor(all_ela_seq)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rii_dataset_train(Dataset):\n",
    "    def __init__(self,train_user):\n",
    "        self.df = train_user\n",
    "        self.ques_seq, self.ans_seq, self.parts_seq, self.ela_seq = get_data_for_train(self.df, 100)\n",
    "    def __len__(self):\n",
    "        return len(self.ques_seq)\n",
    "    def __getitem__(self, index):\n",
    "        return self.ques_seq[index], self.ans_seq[index], self.parts_seq[index], self.ela_seq[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 que_num,\n",
    "                 part_num,\n",
    "                 ela_num,\n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.que_embedding = nn.Embedding(que_num, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.part_embedding = nn.Embedding(part_num, hid_dim)\n",
    "        self.ela_embedding = nn.Embedding(ela_num, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, src_que,src_part,src_ela,src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        batch_size = src_que.shape[0]\n",
    "        src_len = src_que.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        # pos的维度是[batch_size, src_len]，其中每个一维的都是都是[1,100]，\n",
    "        # 其中unsqueeze(0)的作用是将tensor由[seq_len]维度变成[batch_size, seq_len]维\n",
    "        \n",
    "        que_emb = self.que_embedding(src_que)\n",
    "        part_emb = self.part_embedding(src_part)\n",
    "        ela_emb = self.ela_embedding(src_ela)\n",
    "        tok_emb = que_emb+part_emb+ela_emb\n",
    "        \n",
    "        src = self.dropout((tok_emb * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        #self attention\n",
    "#         print(\"In encoder Q shape is:{}\\t K shape is:{}\\t V shape is:{}\\t mask shape is:{}\".format(src.shape,\\\n",
    "#                                                                                                    src.shape,src.shape,src_mask.shape))\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "#         print(\"Q shape is:{}\\t K shape is:{}\\t V shape is:{}\\t energy shape is:{}\\tmask shapeis:{}\".format(Q.shape,\\\n",
    "#                                                                                 K.shape, V.shape, energy.shape,mask.shape))\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 ans_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.ans_embedding = nn.Embedding(ans_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, ans_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.ans_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        #self attention\n",
    "#         print(\"In decoder self atention Q shape is:{}\\t K shape is:{}\\t V shape is:{}\\t trg mask shape is:{}\\t\".format(trg.shape,\\\n",
    "#                                                                                                    trg.shape,trg.shape,trg_mask.shape))\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #encoder attention\n",
    "#         print(\"In encoder-decoder atention Q shape is:{}\\t K shape is:{}\\t V shape is:{}\\t src mask shape is:{}\".format(trg.shape,\\\n",
    "#                                                                                                    enc_src.shape,enc_src.shape,src_mask.shape))\n",
    "\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src_que,src_part,src_ela, trg_ans):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        src_mask = self.make_trg_mask(src_que)\n",
    "        trg_mask = self.make_trg_mask(trg_ans)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src_que,src_part,src_ela,src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg_ans, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_num = 13524\n",
    "ans_num = 4\n",
    "part_num = 9\n",
    "ela_num = 302\n",
    "\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = Encoder(que_num,part_num,ela_num,\n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(ans_num, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_que_idx = 13523\n",
    "trg_pad_ans_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(enc, dec, src_pad_que_idx, trg_pad_ans_idx, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,548,676 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (que_embedding): Embedding(13524, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (part_embedding): Embedding(9, 256)\n",
       "    (ela_embedding): Embedding(302, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (ans_embedding): Embedding(4, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = trg_pad_ans_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    total_num = 0\n",
    "    right_num = 0\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        \n",
    "        src_que, trg_ans, src_part, src_ela = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src_que, src_part, src_ela, trg_ans[:,:-1])\n",
    "        # 由于decoder预测时是错位预测，也就是用trg[t-1]去预测trg[t]，所以输入到decoder模型中的trg缺少最后一个样本的结果 \n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg_ans = trg_ans[:,1:].contiguous().view(-1)\n",
    "        # contiguous()用于判定tensor是否是连续的\n",
    "        \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg_ans)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        \n",
    "        preds = F.softmax(output, dim=-1)\n",
    "        max_vals = \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            print(\"output\")\n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = Rii_dataset_train(train_user.head(100))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2074it [1:20:34,  2.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-994498acfac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-280-cc159ac32d21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch1.4/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch1.4/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-f0cd40d3e413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
