{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDKIM\n",
    "MAX_SEQ = 100\n",
    "#HDKIMHDKIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtype = {'timestamp': 'int64', \n",
    "         'user_id': 'int64' ,\n",
    "         'content_id': 'int16',\n",
    "         'content_type_id': 'int8',\n",
    "         'answered_correctly':'int8',\n",
    "         'prior_question_elapsed_time':'float64'}\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/saktmodel/train_200_valid_2.csv', usecols=[1,2,3,4,7,8], dtype=dtype)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.content_type_id == False]\n",
    "\n",
    "#arrange by timestamp\n",
    "train_df = train_df.sort_values(['timestamp'], ascending=True).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('/kaggle/input/saktmodel/valid_2.csv', usecols=[ 2, 3,4,5,8,9], dtype=dtype)\n",
    "valid = valid[valid.content_type_id == False]\n",
    "\n",
    "valid.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_com = pd.read_csv('/kaggle/input/saktques/question_cmnts.csv')\n",
    "question_com.columns = ['question_id', 'community']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv')\n",
    "question_df['part'].fillna(8, inplace=True)\n",
    "question_df['part'] = question_df['part']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df = pd.merge(question_df, question_com, on='question_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df.loc[question_df['community']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del question_com\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_skill = question_df['part'].unique()\n",
    "print(\"part is:\",part_skill)\n",
    "n_part_skill = len(part_skill)\n",
    "print(\"number part skills\", n_part_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_skill = question_df['community'].unique()\n",
    "print(\"community is:\", com_skill)\n",
    "n_com_skill = len(com_skill)\n",
    "print(\"number community skills is:\", n_com_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_mean = train_df.prior_question_elapsed_time.mean()\n",
    "elapsed_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elapsed_time(ela):\n",
    "    ela = ela // 1000\n",
    "    if ela > 300:\n",
    "        return 300\n",
    "    else:\n",
    "        return ela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['prior_question_elapsed_time'] = train_df['prior_question_elapsed_time'].apply(lambda x: get_elapsed_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, question_df, left_on='content_id',right_on='question_id',how='left')\n",
    "valid = pd.merge(valid, question_df, left_on='content_id',right_on='question_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skills = train_df[\"content_id\"].unique()\n",
    "n_skill = 13523\n",
    "print(\"number skills\", n_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ela_skill = train_df['prior_question_elapsed_time'].unique()\n",
    "n_ela_skill = 301\n",
    "print(\"number ela skills\", n_ela_skill)\n",
    "print(\"ela is:\", ela_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['prior_question_elapsed_time'].fillna(elapsed_mean, inplace=True)\n",
    "valid['prior_question_elapsed_time'] = valid['prior_question_elapsed_time'].apply(lambda x: get_elapsed_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = train_df[['user_id', 'part','content_id','community', 'answered_correctly','prior_question_elapsed_time']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values,\n",
    "            r['prior_question_elapsed_time'].values,\n",
    "            r['part'].values,\n",
    "            r['community'].values))\n",
    "\n",
    "\n",
    "\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDKIM\n",
    "import random\n",
    "random.seed(1)\n",
    "#HDKIMHDKIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAKTDataset(Dataset):\n",
    "    def __init__(self, group, n_skill,n_ela_skill, max_seq=MAX_SEQ): #HDKIM 100\n",
    "        super(SAKTDataset, self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.n_skill = n_skill\n",
    "        self.n_ela_skill = n_ela_skill\n",
    "        self.samples = group\n",
    "        \n",
    "#         self.user_ids = [x for x in group.index]\n",
    "        self.user_ids = []\n",
    "        for user_id in group.index:\n",
    "            q, qa,ela,part,com = group[user_id]\n",
    "            if len(q) < 2: #HDKIM 10\n",
    "                continue\n",
    "            self.user_ids.append(user_id)\n",
    "            \n",
    "            #HDKIM Memory reduction\n",
    "#             if len(q)>self.max_seq:\n",
    "#                 group[user_id] = (q[-self.max_seq:],qa[-self.max_seq:], ela[-self.max_seq:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = self.user_ids[index]\n",
    "        q_, qa_,ela_,part_,com_ = self.samples[user_id]\n",
    "        seq_len = len(q_)\n",
    "\n",
    "        q = np.ones(self.max_seq, dtype=int)*13523\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        ela = np.ones(self.max_seq, dtype=int)*301\n",
    "        part = np.ones(self.max_seq, dtype=int)*7\n",
    "        com = np.ones(self.max_seq, dtype=int)*4\n",
    "\n",
    "        if seq_len >= self.max_seq:\n",
    "            if random.random() >0.1:\n",
    "                start = random.randint(0, (seq_len-self.max_seq))\n",
    "                end = start + self.max_seq\n",
    "                q[:] = q_[start:end]\n",
    "                qa[:] = qa_[start:end]\n",
    "                ela[:] = ela_[start:end]\n",
    "                part[:] = part_[start:end]\n",
    "                com[:] = com_[start:end]\n",
    "\n",
    "            else:   \n",
    "                q[:] = q_[-self.max_seq:]\n",
    "                qa[:] = qa_[-self.max_seq:]\n",
    "                ela[:] = ela_[-self.max_seq:]\n",
    "                part[:] = part_[-self.max_seq:]\n",
    "                com[:] = com_[-self.max_seq:]\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            if random.random() > 0.1:\n",
    "                start = 0\n",
    "                end = random.randint(2, seq_len)\n",
    "                seq_len = end -start\n",
    "                q[-seq_len:] = q_[0:seq_len]\n",
    "                qa[-seq_len:] = qa_[0:seq_len]\n",
    "                ela[-seq_len:] = ela_[0:seq_len]\n",
    "                part[-seq_len:] = part_[0:seq_len]\n",
    "                com[-seq_len:] = com_[0:seq_len]\n",
    "                \n",
    "            else:\n",
    "                q[-seq_len:] = q_\n",
    "                qa[-seq_len:] = qa_\n",
    "                ela[-seq_len:] = ela_   \n",
    "                part[-seq_len:] = part_   \n",
    "                com[-seq_len:] = com_   \n",
    "            \n",
    "        label = qa[:]\n",
    "        \n",
    "        # encoder输入，从0开始一直到len \n",
    "        enc_que = np.ones(self.max_seq, dtype=int)*13523\n",
    "        enc_que = q[:].copy()\n",
    "        \n",
    "        enc_part = np.ones(self.max_seq, dtype=int)*7\n",
    "        enc_part = part[:].copy()\n",
    "        \n",
    "        enc_com = np.ones(self.max_seq, dtype=int)*4\n",
    "        enc_com = com[:].copy()\n",
    "        \n",
    "        # decoder输入， 先在左边pad一个位置，然后从0一直到len-1\n",
    "        dec_ela = np.ones(self.max_seq, dtype=int)*301\n",
    "        dec_ela[1:] = ela[:-1].copy()\n",
    "        \n",
    "        dec_ans = np.zeros(self.max_seq, dtype=int)\n",
    "        dec_ans[1:] = qa[:-1].copy()\n",
    "        \n",
    "\n",
    "        \n",
    "        return enc_que, enc_part, enc_com, dec_ans,  label\n",
    "        \n",
    "        \n",
    "#         return x, target_id,ela_x,ela_target,part_x,part_target,com_x,com_target,ans_x,label\n",
    "    # x和target_id都是来自于q，存在一个错位，x比target_id提前一个位置\n",
    "    # 所以我们可以尝试将ela 也分为ela_x和ela_target_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SAKTDataset(group, n_skill, n_ela_skill)\n",
    "dataloader = DataLoader(dataset, batch_size=2048, shuffle=True,num_workers=8)\n",
    "\n",
    "item = dataset.__getitem__(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, samples, test_df, n_skill,n_ela_skill, max_seq=MAX_SEQ): #HDKIM 100\n",
    "        super(ValidDataset, self).__init__()\n",
    "        self.samples = samples\n",
    "        self.user_ids = [x for x in test_df[\"user_id\"].unique()]\n",
    "        self.test_df = test_df\n",
    "        self.n_ela_skill = n_ela_skill\n",
    "        self.n_skill = n_skill\n",
    "        self.max_seq = max_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        test_info = self.test_df.iloc[index]\n",
    "\n",
    "        user_id = test_info[\"user_id\"]\n",
    "        enc_target_que = test_info[\"content_id\"]\n",
    "#         dec_target_ela = test_info[\"prior_question_elapsed_time\"]\n",
    "        enc_target_part = test_info[\"part\"]\n",
    "        enc_target_com = test_info[\"community\"]\n",
    "\n",
    "        \n",
    "        label = test_info['answered_correctly']\n",
    "\n",
    "        q = np.ones(self.max_seq, dtype=int)*13523\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        ela = np.ones(self.max_seq, dtype=int)*301\n",
    "        part = np.ones(self.max_seq, dtype=int)*7\n",
    "        com = np.ones(self.max_seq, dtype=int)*4\n",
    "\n",
    "        if user_id in self.samples.index:\n",
    "            q_, qa_, ela_, part_,com_ = self.samples[user_id]\n",
    "            \n",
    "            seq_len = len(q_)\n",
    "\n",
    "            if seq_len >= self.max_seq:\n",
    "                q = q_[-self.max_seq:]\n",
    "                qa = qa_[-self.max_seq:]\n",
    "                ela = ela_[-self.max_seq:]\n",
    "                part = part_[-self.max_seq:]\n",
    "                com = com_[-self.max_seq:]\n",
    "                \n",
    "            else:\n",
    "                q[-seq_len:] = q_\n",
    "                qa[-seq_len:] = qa_       \n",
    "                ela[-seq_len:] = ela_\n",
    "                part[-seq_len:] = part_\n",
    "                com[-seq_len:] = com_\n",
    "        \n",
    "        \n",
    "        # encoder的输入\n",
    "        enc_que = np.ones(self.max_seq, dtype=int)*13523\n",
    "        enc_que = np.append(q[1:].copy(),[enc_target_que])\n",
    "        \n",
    "        enc_part = np.ones(self.max_seq, dtype=int)*7\n",
    "        enc_part = np.append(part[1:].copy(), [enc_target_part])\n",
    "        \n",
    "        enc_com = np.ones(self.max_seq, dtype=int)*4\n",
    "        enc_com = np.append(com[1:].copy(), [enc_target_com])\n",
    "        \n",
    "        # decoder的输入\n",
    "        dec_ans = np.zeros(self.max_seq, dtype=int)\n",
    "        dec_ans = qa[:].copy()\n",
    "        \n",
    "\n",
    "\n",
    "        return enc_que, enc_part, enc_com, dec_ans, np.array([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ValidDataset(group, valid, n_skill, n_ela_skill)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2048, shuffle=False,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_mask(seq_length):\n",
    "    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "    return torch.from_numpy(future_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=200):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, device=\"cpu\"):\n",
    "        super().__init__( )\n",
    "        \n",
    "        self.device = device\n",
    "        self.multi_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, enc_que, enc_part, enc_com, enc_pos):\n",
    "        out = enc_que + enc_part + enc_com + enc_pos\n",
    "        \n",
    "        out = out.permute(1,0,2)\n",
    "        \n",
    "        # Multihead attention\n",
    "        n,_,_ = out.shape\n",
    "        out = self.layer_norm1(out)\n",
    "        skip_out = out\n",
    "        out, attn = self.multi_attn(out, out, out, \n",
    "                                    attn_mask=future_mask(seq_length=n).to(self.device))\n",
    "            \n",
    "        out = out + skip_out\n",
    "        \n",
    "        # FeedForward\n",
    "        out = out.permute(1,0,2)\n",
    "        out = self.layer_norm2(out)\n",
    "        skip_out = out\n",
    "        out = self.ffn(out)\n",
    "        out = out + skip_out\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.multi_attn1 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.multi_attn2 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, enc_out, dec_ans, dec_pos):\n",
    "        out = dec_ans + dec_pos\n",
    "        \n",
    "        out = out.permute(1,0,2)\n",
    "        n,_,_ = out.shape\n",
    "        \n",
    "        # Decoder slef Multihead Attention \n",
    "        out = self.layer_norm1(out)\n",
    "        skip_out = out\n",
    "        out, attn = self.multi_attn1(out, out, out,\n",
    "                                     attn_mask=future_mask(seq_length=n).to(self.device))\n",
    "        \n",
    "        out = out + skip_out\n",
    "        \n",
    "        # Encoder-Decoder Multihead Attention\n",
    "        enc_out = enc_out.permute(1,0,2)\n",
    "#         enc_out = self.layer_norm2(enc_out)\n",
    "        enc_out = self.layer_norm2(enc_out)\n",
    "        \n",
    "        skip_out = out\n",
    "        \n",
    "        out, attn = self.multi_attn2(out, enc_out, enc_out,\n",
    "                                     attn_mask=future_mask(seq_length=n).to(self.device))\n",
    "        out = out + skip_out\n",
    "        \n",
    "        \n",
    "        # FeedForward \n",
    "        out = out.permute(1,0,2)\n",
    "#         out = self.layer_norm3(out)\n",
    "        out = self.layer_norm3(out)\n",
    "        \n",
    "        skip_out = out\n",
    "        out = self.ffn(out)\n",
    "        out = out + skip_out\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaintModel(nn.Module):\n",
    "    def __init__(self,num_que, num_part, num_com, num_ans=2, num_heads=8, \n",
    "                 max_seq=MAX_SEQ, embed_dim=128, device = \"cpu\"):\n",
    "        super(SaintModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.num_que = num_que\n",
    "        self.num_part = num_part\n",
    "        self.num_com = num_com\n",
    "        self.num_ans = num_ans\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.que_embedding = nn.Embedding(num_que, embed_dim)\n",
    "        self.part_embedding = nn.Embedding(num_part, embed_dim)\n",
    "        self.com_embedding = nn.Embedding(num_com, embed_dim)\n",
    "        self.ans_embedding = nn.Embedding(num_ans, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq+1, embed_dim)\n",
    "        \n",
    "        self.encoder = EncoderBlock(embed_dim, num_heads, device)\n",
    "        self.decoder = DecoderBlock(embed_dim, num_heads, device)\n",
    "        \n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, enc_que, enc_part, enc_com, dec_ans):\n",
    "#         device = enc_que.device\n",
    "        \n",
    "        enc_que = self.que_embedding(enc_que)\n",
    "        enc_part = self.part_embedding(enc_part)\n",
    "        enc_com = self.com_embedding(enc_com)\n",
    "        dec_ans = self.ans_embedding(dec_ans)\n",
    "        \n",
    "        enc_pos = torch.arange(1,1+enc_que.size(1)).unsqueeze(0).to(self.device)\n",
    "        dec_pos = torch.arange(enc_que.size(1)).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        enc_pos = self.pos_embedding(enc_pos)\n",
    "        dec_pos = self.pos_embedding(dec_pos)\n",
    "        \n",
    "        \n",
    "        enc_out = self.encoder(enc_que, enc_part, enc_com, enc_pos)\n",
    "        dec_out = self.decoder(enc_out, dec_ans, dec_pos)\n",
    "        \n",
    "        out = self.pred(dec_out)\n",
    "        \n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = xm.xla_device()\n",
    "\n",
    "model = SaintModel(n_skill+1,n_part_skill+1,n_com_skill, embed_dim=128, device=device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=0.005)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iterator, optim, criterion, device=\"cpu\"):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    tbar = tqdm(train_iterator)\n",
    "    for item in tbar:\n",
    "        enc_que = item[0].to(device).long()\n",
    "        enc_part = item[1].to(device).long()\n",
    "        enc_com = item[2].to(device).long()\n",
    "        dec_ans = item[3].to(device).long()\n",
    "        \n",
    "        label = item[4].to(device).float()        \n",
    "        \n",
    "        optim.zero_grad()\n",
    "        output = model(enc_que, enc_part, enc_com, dec_ans)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "#         xm.mark_step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        output = output[:, -1]\n",
    "        label = label[:, -1] \n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "        tbar.set_description('loss - {:.4f}'.format(loss))\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(train_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, test_dataloader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    eval_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(test_dataloader):\n",
    "            enc_que = item[0].to(device).long()\n",
    "            enc_part = item[1].to(device).long()\n",
    "            enc_com = item[2].to(device).long()\n",
    "            dec_ans = item[3].to(device).long()\n",
    "        \n",
    "            label = item[4].to(device).float()        \n",
    "        \n",
    "#             print(\"In evaluation x shape is:{}\\t taret_id shape is:{}\\t label shape is:{}\".format(x.shape, target_id.shape,label.shape))\n",
    "            \n",
    "            output = model(enc_que, enc_part, enc_com, dec_ans)\n",
    "#             print(\"Output shape is:{}\\tlabel shape is:{}\".format(output.shape, label.shape))\n",
    "      \n",
    "            output_loss = output[:, -1:]\n",
    "            loss = criterion(output_loss, label)\n",
    "            eval_loss.append(loss.item())\n",
    "            \n",
    "            output = output[:,-1]\n",
    "            label = label[:, -1] \n",
    "            pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "            \n",
    "            num_corrects += (pred == label).sum().item()\n",
    "            num_total += len(label)\n",
    "\n",
    "            labels.extend(label.view(-1).data.cpu().numpy())\n",
    "            outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "#     print(\"In evaluate labels is:{}\\t outs is:{}\\t\".format(labels, outs))\n",
    "\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "#     auc = 0\n",
    "\n",
    "    loss = np.mean(eval_loss)   \n",
    "    \n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "best_auc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss, acc, auc = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "    print(\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.3f}\".format(epoch, loss, acc, auc))\n",
    "    \n",
    "    if epoch %10 == 0 or epoch == 79:\n",
    "        eval_loss, eval_acc, eval_auc = evaluate_epoch(model, test_dataloader, criterion, device)\n",
    "        print(\"epoch - {} eval_loss - {:.2f} eval acc - {:.3f} eval auc - {:.3f}\".format(epoch, eval_loss, eval_acc, eval_auc))\n",
    "        if best_auc < eval_auc: \n",
    "            best_auc = eval_auc\n",
    "            torch.save(model.state_dict(),'best_model.pt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, samples, test_df, n_skill, n_ela_skill,n_part_skill,max_seq=MAX_SEQ): #HDKIM 100\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.samples = samples\n",
    "        self.user_ids = [x for x in test_df[\"user_id\"].unique()]\n",
    "        self.test_df = test_df\n",
    "        self.n_ela_skill = n_ela_skill\n",
    "        self.n_part_skill = n_part_skill\n",
    "        self.n_skill = n_skill\n",
    "        self.max_seq = max_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        test_info = self.test_df.iloc[index]\n",
    "\n",
    "        user_id = test_info[\"user_id\"]\n",
    "        target_id = test_info[\"content_id\"]\n",
    "        ela_target_id = test_info[\"prior_question_elapsed_time\"]\n",
    "        part_target_id = test_info[\"part\"]\n",
    "        com_target_id = test_info[\"community\"]\n",
    "\n",
    "        q = np.ones(self.max_seq, dtype=int)*13523\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        ela = np.ones(self.max_seq, dtype=int)*301\n",
    "        part = np.ones(self.max_seq, dtype=int)*7\n",
    "        com = np.ones(self.max_seq, dtype=int)*4\n",
    "\n",
    "        if user_id in self.samples.index:\n",
    "#             print(\"self.samples[user_id] is:{}\\n length is:{}\".format(\n",
    "#                 self.samples[user_id], len(self.samples[user_id])))\n",
    "            q_, qa_, ela_,part_,com_ = self.samples[user_id]\n",
    "            seq_len = len(q_)\n",
    "\n",
    "            if seq_len >= self.max_seq:\n",
    "                q = q_[-self.max_seq:]\n",
    "                qa = qa_[-self.max_seq:]\n",
    "                ela = ela_[-self.max_seq:]\n",
    "                part = part_[-self.max_seq:]\n",
    "                com = com_[-self.max_seq:]\n",
    "\n",
    "            else:\n",
    "                q[-seq_len:] = q_\n",
    "                qa[-seq_len:] = qa_          \n",
    "                ela[-seq_len:] = ela_\n",
    "                part[-seq_len:] = part_\n",
    "                com[-seq_len:] = com_\n",
    "\n",
    "                \n",
    "        x = np.ones(self.max_seq-1, dtype=int)*13523\n",
    "        x = q[1:].copy()\n",
    "#         x += (qa[1:] == 1) * self.n_skill\n",
    "        \n",
    "        questions = np.append(q[2:], [target_id])\n",
    "        \n",
    "        ela_x = np.ones(self.max_seq-1, dtype=int)*301\n",
    "        ela_x = ela[1:].copy()\n",
    "#         ela_x += (qa[1:] == 1) * self.n_ela_skill\n",
    "        \n",
    "        ela_target = np.append(ela[2:],[ela_target_id])  \n",
    "        \n",
    "        part_x = np.ones(self.max_seq-1, dtype=int)*7\n",
    "        part_x = part[1:].copy()\n",
    "        \n",
    "        part_target = np.append(part[2:],[part_target_id])     \n",
    "        \n",
    "        com_x = np.ones(self.max_seq-1, dtype=int)*4\n",
    "        com_x = com[1:].copy()\n",
    "        \n",
    "        com_target = np.append(com[2:],[com_target_id])        \n",
    "        \n",
    "        ans_x = np.zeros(self.max_seq-1, dtype=int)\n",
    "        ans_x = qa[1:].copy()\n",
    "        return torch.LongTensor(x), torch.LongTensor(questions),\\\n",
    "            torch.LongTensor(ela_x), torch.LongTensor(ela_target),\\\n",
    "            torch.LongTensor(part_x), torch.LongTensor(part_target),\\\n",
    "            torch.LongTensor(com_x), torch.LongTensor(com_target),\\\n",
    "            torch.LongTensor(ans_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAKTModel(n_skill+1,n_ela_skill+1,n_part_skill+1,n_com_skill,embed_dim=128)\n",
    "model.load_state_dict(torch.load('best_model.pt',map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()\n",
    "import psutil\n",
    "model.eval()\n",
    "\n",
    "#HDKIM\n",
    "prev_test_df = None\n",
    "#HDKIMHDKIM\n",
    "\n",
    "for (test_df, sample_prediction_df) in tqdm(iter_test):\n",
    "    #HDKIM\n",
    "    if (prev_test_df is not None) & (psutil.virtual_memory().percent<90):\n",
    "        print(psutil.virtual_memory().percent)\n",
    "        prev_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])\n",
    "        prev_test_df = prev_test_df[prev_test_df.content_type_id == False]\n",
    "        prev_group = prev_test_df[['user_id', 'part','content_id','community','answered_correctly','prior_question_elapsed_time']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values,\n",
    "            r['prior_question_elapsed_time'].values,\n",
    "            r['part'].values,\n",
    "            r['community'].values))\n",
    "        for prev_user_id in prev_group.index:\n",
    "            prev_group_content = prev_group[prev_user_id][0]\n",
    "            prev_group_ac = prev_group[prev_user_id][1]\n",
    "            prev_group_ela = prev_group[prev_user_id][2]\n",
    "            prev_group_part = prev_group[prev_user_id][3]\n",
    "            prev_group_com = prev_group[prev_user_id][4]\n",
    "\n",
    "            if prev_user_id in group.index:\n",
    "                group[prev_user_id] = (np.append(group[prev_user_id][0],prev_group_content), \n",
    "                                       np.append(group[prev_user_id][1],prev_group_ac),\n",
    "                                       np.append(group[prev_user_id][2],prev_group_ela),\n",
    "                                       np.append(group[prev_user_id][3],prev_group_part),\n",
    "                                       np.append(group[prev_user_id][4],prev_group_com))\n",
    " \n",
    "            else:\n",
    "                group[prev_user_id] = (prev_group_content,prev_group_ac, prev_group_ela,prev_group_part,prev_group_com)\n",
    "            if len(group[prev_user_id][0])>MAX_SEQ:\n",
    "                new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n",
    "                new_group_ac = group[prev_user_id][1][-MAX_SEQ:]\n",
    "                new_group_ela = group[prev_user_id][2][-MAX_SEQ:]\n",
    "                new_group_part = group[prev_user_id][3][-MAX_SEQ:]\n",
    "                new_group_com = group[prev_user_id][4][-MAX_SEQ:]\n",
    "\n",
    "                group[prev_user_id] = (new_group_content,new_group_ac,new_group_ela,new_group_part,new_group_com)\n",
    "\n",
    "    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace=True)\n",
    "    test_df['prior_question_elapsed_time'] = test_df['prior_question_elapsed_time'].apply(lambda x: get_elapsed_time(x))\n",
    "             \n",
    "    test_df = pd.merge(test_df, question_df, left_on='content_id',right_on='question_id',how='left')\n",
    "    prev_test_df = test_df.copy()\n",
    " \n",
    "    #HDKIMHDKIM\n",
    "    \n",
    "    test_df = test_df[test_df.content_type_id == False]\n",
    "   \n",
    "    test_dataset = TestDataset(group, test_df, n_skill, n_ela_skill, n_part_skill)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=51200, \n",
    "                                 shuffle=False)\n",
    "    \n",
    "    outs = []\n",
    "\n",
    "    for item in tqdm(test_dataloader):\n",
    "        x = item[0].to(device).long()\n",
    "        target_id = item[1].to(device).long()\n",
    "        ela_x = item[2].to(device).long()\n",
    "        ela_target = item[3].to(device).long()\n",
    "        part_x = item[4].to(device).long()\n",
    "        part_target = item[5].to(device).long()  \n",
    "        com_x = item[6].to(device).long()\n",
    "        com_target = item[7].to(device).long() \n",
    "        \n",
    "        ans_x = item[8].to(device).long()  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, att_weight = model(x, target_id,ela_x, ela_target,\n",
    "                                       part_x, part_target,com_x, com_target,ans_x)\n",
    "        \n",
    "        \n",
    "        output = torch.sigmoid(output)\n",
    "        output = output[:, -1]\n",
    "\n",
    "        # pred = (output >= 0.5).long()\n",
    "        # loss = criterion(output, label)\n",
    "\n",
    "        # val_loss.append(loss.item())\n",
    "        # num_corrects += (pred == label).sum().item()\n",
    "        # num_total += len(label)\n",
    "\n",
    "        # labels.extend(label.squeeze(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "        \n",
    "    test_df['answered_correctly'] =  outs\n",
    "    \n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
