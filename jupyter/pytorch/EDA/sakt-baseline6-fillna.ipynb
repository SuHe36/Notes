{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:15.917370Z",
     "iopub.status.busy": "2020-11-17T03:20:15.916442Z",
     "iopub.status.idle": "2020-11-17T03:20:17.075144Z",
     "shell.execute_reply": "2020-11-17T03:20:17.074301Z"
    },
    "papermill": {
     "duration": 1.198565,
     "end_time": "2020-11-17T03:20:17.075282",
     "exception": false,
     "start_time": "2020-11-17T03:20:15.876717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "env = riiideducation.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:17.147197Z",
     "iopub.status.busy": "2020-11-17T03:20:17.146203Z",
     "iopub.status.idle": "2020-11-17T03:20:17.149797Z",
     "shell.execute_reply": "2020-11-17T03:20:17.149138Z"
    },
    "papermill": {
     "duration": 0.040645,
     "end_time": "2020-11-17T03:20:17.149955",
     "exception": false,
     "start_time": "2020-11-17T03:20:17.109310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/riiid-test-answer-prediction/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031671,
     "end_time": "2020-11-17T03:20:17.213675",
     "exception": false,
     "start_time": "2020-11-17T03:20:17.182004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ARGS参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:17.287833Z",
     "iopub.status.busy": "2020-11-17T03:20:17.286736Z",
     "iopub.status.idle": "2020-11-17T03:20:17.290445Z",
     "shell.execute_reply": "2020-11-17T03:20:17.289837Z"
    },
    "papermill": {
     "duration": 0.04411,
     "end_time": "2020-11-17T03:20:17.290573",
     "exception": false,
     "start_time": "2020-11-17T03:20:17.246463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    name = 'train'\n",
    "    model = 'SAKT'\n",
    "    num_layers = 1\n",
    "    hidden_dim=100\n",
    "    input_dim = 100\n",
    "    dropout = 0.2\n",
    "    num_head = 5\n",
    "    \n",
    "    random_seed = 1\n",
    "    num_epochs = 1\n",
    "    lr = 0.01\n",
    "    seq_size = 20\n",
    "    warm_up_step_count = 4000\n",
    "    eval_steps = 50000\n",
    "    train_steps = 50000\n",
    "    train_batch=500\n",
    "    test_batch=256\n",
    "    num_workers=1\n",
    "    device='cpu'\n",
    "    \n",
    "    \n",
    "ARGS = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:17.362198Z",
     "iopub.status.busy": "2020-11-17T03:20:17.361122Z",
     "iopub.status.idle": "2020-11-17T03:20:17.364884Z",
     "shell.execute_reply": "2020-11-17T03:20:17.364244Z"
    },
    "papermill": {
     "duration": 0.040831,
     "end_time": "2020-11-17T03:20:17.365033",
     "exception": false,
     "start_time": "2020-11-17T03:20:17.324202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAD_INDEX = 0\n",
    "\n",
    "QUESTION_NUM = {\n",
    "    'riii':13523\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:17.438544Z",
     "iopub.status.busy": "2020-11-17T03:20:17.437703Z",
     "iopub.status.idle": "2020-11-17T03:20:18.512468Z",
     "shell.execute_reply": "2020-11-17T03:20:18.513048Z"
    },
    "papermill": {
     "duration": 1.114767,
     "end_time": "2020-11-17T03:20:18.513227",
     "exception": false,
     "start_time": "2020-11-17T03:20:17.398460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "from itertools import repeat, chain, islice\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:18.603917Z",
     "iopub.status.busy": "2020-11-17T03:20:18.598408Z",
     "iopub.status.idle": "2020-11-17T03:20:18.639049Z",
     "shell.execute_reply": "2020-11-17T03:20:18.639597Z"
    },
    "papermill": {
     "duration": 0.08804,
     "end_time": "2020-11-17T03:20:18.639778",
     "exception": false,
     "start_time": "2020-11-17T03:20:18.551738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UserSepDataSet_Rii_Train(Dataset):\n",
    "\n",
    "    def __init__(self,train_df, question_df):\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.question_df = question_df\n",
    "\n",
    "        self.combine = self.get_combine()\n",
    "        self.question_ids, self.target_ids, self.labels = self.get_data_for_train()\n",
    "\n",
    "\n",
    "    def get_combine(self):\n",
    "        # self.train_df['content_id'] = self.train_df['content_id'] + 1\n",
    "        self.train_df = pd.merge(self.train_df, self.question_df,\n",
    "                                 left_on=\"content_id\",right_on='question_id', how='left')\n",
    "        self.train_df.drop(['content_id'], axis=1)\n",
    "        self.train_df = self.train_df[self.train_df.content_type_id == False]\n",
    "        self.train_df = self.train_df.sort_values(['timestamp'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "        self.train_df[\"question_id\"] = self.train_df[\"question_id\"].astype('str')\n",
    "        # self.train_df[\"user_id\"] = self.train_df[\"user_id\"].astype('str')\n",
    "        self.train_df[\"answered_correctly\"] = self.train_df[\"answered_correctly\"].astype('str')\n",
    "#         self.train_df[\"prior_question_elapsed_time\"] = self.train_df[\"prior_question_elapsed_time\"].astype('str')\n",
    "#         self.train_df[\"prior_question_had_explanation\"] = self.train_df[\"prior_question_had_explanation\"].astype('str')\n",
    "#         self.train_df[\"part\"] = self.train_df[\"part\"].astype('str')\n",
    "        self.train_df[\"timestamp\"] = self.train_df[\"timestamp\"].astype('str')\n",
    "\n",
    "        combine = self.train_df.groupby(by='user_id').agg({'question_id': ','.join, 'answered_correctly': ','.join,\n",
    "                        'timestamp':','.join}).reset_index()\n",
    "\n",
    "        return combine\n",
    "\n",
    "    def get_data_for_train(self):\n",
    "        all_user_ids = []\n",
    "        all_question_ids = []\n",
    "        all_labels= []\n",
    "        all_target_qids = []\n",
    "\n",
    "        for row in self.combine.itertuples():\n",
    "            user_id = getattr(row, 'user_id')\n",
    "            question_ids = getattr(row, 'question_id').strip().split(',')\n",
    "            answers = getattr(row, 'answered_correctly').strip().split(',')\n",
    "#             part_ids = getattr(row, 'part').strip().split(',')\n",
    "#             elapseds = getattr(row, 'prior_question_elapsed_time').strip().split(',')\n",
    "            # print(\"the length is:\",(len(content_ids), len(answers), len(parts),len(elapseds)))\n",
    "            assert len(question_ids) == len(answers)\n",
    "\n",
    "            all_user_ids.append(user_id)\n",
    "\n",
    "            for target_index in range(0, len(question_ids)):\n",
    "                qids = question_ids[:target_index + 1]\n",
    "                ans_flags = answers[:target_index + 1]\n",
    "\n",
    "                length = len(qids)\n",
    "                if length > ARGS.seq_size + 1:\n",
    "                    qids = qids[-(ARGS.seq_size + 1):]\n",
    "                    ans_flags = ans_flags[-(ARGS.seq_size + 1):]\n",
    "                    pad_counts = 0\n",
    "                else:\n",
    "                    pad_counts = ARGS.seq_size + 1 - length\n",
    "\n",
    "                input_list = []\n",
    "                for idx in range(len(qids)):\n",
    "                    # print(f\"idx is:{idx}, qids[idx] is:{qids[idx]}\")\n",
    "                    tag_id = int(float(qids[idx].strip()))\n",
    "                    is_correct = int(float(ans_flags[idx].strip()))\n",
    "\n",
    "                    if idx == len(qids) - 1:\n",
    "                        last_is_correct = is_correct\n",
    "                        target_id = tag_id\n",
    "                    else:\n",
    "                        if is_correct:\n",
    "                            input_list.append(tag_id)\n",
    "                        else:\n",
    "                            input_list.append(tag_id + QUESTION_NUM['riii'])\n",
    "\n",
    "                paddings = [PAD_INDEX]*pad_counts\n",
    "                input_list = paddings + input_list\n",
    "                assert len(input_list) == ARGS.seq_size\n",
    "\n",
    "                all_question_ids.append(input_list)\n",
    "                all_target_qids.append([target_id])\n",
    "                all_labels.append([last_is_correct])\n",
    "\n",
    "        return torch.LongTensor(all_question_ids), \\\n",
    "            torch.LongTensor(all_target_qids),\\\n",
    "            torch.LongTensor(all_labels)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.question_ids[index], self.target_ids[index], self.labels[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:18.726566Z",
     "iopub.status.busy": "2020-11-17T03:20:18.719535Z",
     "iopub.status.idle": "2020-11-17T03:20:18.850503Z",
     "shell.execute_reply": "2020-11-17T03:20:18.849817Z"
    },
    "papermill": {
     "duration": 0.177863,
     "end_time": "2020-11-17T03:20:18.850633",
     "exception": false,
     "start_time": "2020-11-17T03:20:18.672770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UserSepDataSet_Rii_Test(Dataset):\n",
    "\n",
    "    def __init__(self, test_df, question_df, train_combine, is_test=True):\n",
    "        self.test_df = test_df\n",
    "        self.question_df = question_df\n",
    "        self.train_combine = train_combine\n",
    "        self.is_test = is_test\n",
    "        if self.is_test:\n",
    "            self.question_ids, self.target_ids = self.get_data_for_test()\n",
    "        else:\n",
    "            self.question_ids, self.target_ids, self.labels = self.get_data_for_valid()\n",
    "\n",
    "    def get_index(self, nums, target):\n",
    "        left, right = 0, len(nums)-1\n",
    "\n",
    "        while left <= right:\n",
    "            mid = (right-left)//2 + left\n",
    "            if nums[mid] == target:\n",
    "                return mid\n",
    "            elif nums[mid] > target:\n",
    "                right = mid -1\n",
    "            else:\n",
    "                left = mid + 1\n",
    "\n",
    "        return left\n",
    "\n",
    "    def get_data_for_test(self):\n",
    "        all_question_ids = []\n",
    "        all_target_ids = []\n",
    "\n",
    "        for row in self.test_df.itertuples():\n",
    "            user_id = getattr(row, 'user_id')\n",
    "            q_id = getattr(row, 'content_id')\n",
    "            timestamp = getattr(row, 'timestamp')\n",
    "            # ans = getattr(row, 'answered_correctly')\n",
    "            # last_is_correct = ans\n",
    "            target_id = int(float(q_id))\n",
    "\n",
    "            train_user = self.train_combine.loc[self.train_combine['user_id']==user_id]\n",
    "            if train_user.empty == False:\n",
    "                timestamps = train_user['timestamp'].item().strip().split(',')\n",
    "                timestamps = [int(float(e.strip())) for e in timestamps]\n",
    "\n",
    "                q_ids = train_user['question_id'].item().strip().split(',')\n",
    "                q_ids = [int(float(e.strip())) for e in q_ids]\n",
    "\n",
    "                answers = train_user['answered_correctly'].item().strip().split(',')\n",
    "                answers = [int(float(e.strip())) for e in answers]\n",
    "\n",
    "                assert len(q_ids) == len(timestamps) == len(answers)\n",
    "\n",
    "                target_index = self.get_index(timestamps, timestamp)\n",
    "                if target_index == 0:\n",
    "                    input_list = [PAD_INDEX] * ARGS.seq_size\n",
    "                else:\n",
    "                    qids = q_ids[:target_index]\n",
    "                    ans_flags = answers[:target_index]\n",
    "\n",
    "                    length = len(qids)\n",
    "                    if length > ARGS.seq_size:\n",
    "                        qids = qids[-ARGS.seq_size:]\n",
    "                        ans_flags =  ans_flags[-ARGS.seq_size:]\n",
    "                        pad_counts = 0\n",
    "                    else:\n",
    "                        pad_counts = ARGS.seq_size - length\n",
    "\n",
    "                    input_list = []\n",
    "                    for idx in range(len(qids)):\n",
    "                        tag_id = qids[idx]\n",
    "                        is_correct = ans_flags[idx]\n",
    "\n",
    "                        if is_correct:\n",
    "                            input_list.append(tag_id)\n",
    "                        else:\n",
    "                            input_list.append(tag_id + QUESTION_NUM['riii'])\n",
    "\n",
    "                    paddings = [PAD_INDEX]*pad_counts\n",
    "                    input_list = paddings + input_list\n",
    "                    assert len(input_list) == ARGS.seq_size\n",
    "\n",
    "            else:\n",
    "                input_list = [PAD_INDEX] * ARGS.seq_size\n",
    "\n",
    "            all_question_ids.append(input_list)\n",
    "            all_target_ids.append([target_id])\n",
    "\n",
    "        return torch.LongTensor(all_question_ids), \\\n",
    "            torch.LongTensor(all_target_ids),\n",
    "\n",
    "\n",
    "\n",
    "    def get_data_for_valid(self):\n",
    "        all_question_ids = []\n",
    "        all_labels = []\n",
    "        all_target_ids = []\n",
    "\n",
    "        for row in self.test_df.itertuples():\n",
    "            user_id = getattr(row, 'user_id')\n",
    "            q_id = getattr(row, 'content_id')\n",
    "            timestamp = getattr(row, 'timestamp')\n",
    "            ans = getattr(row, 'answered_correctly')\n",
    "            last_is_correct = ans\n",
    "            target_id = int(float(q_id))\n",
    "            if target_id > 13523:\n",
    "                print(\"user_id:{}, q_id:{}\".format(user_id, q_id))\n",
    "\n",
    "            train_user = self.train_combine.loc[self.train_combine['user_id']==user_id]\n",
    "            c = train_user\n",
    "            if train_user.empty == False:\n",
    "                # print(\"Bingo\")\n",
    "                timestamps = train_user['timestamp'].item().strip().split(',')\n",
    "                timestamps = [int(float(e.strip())) for e in timestamps]\n",
    "\n",
    "                q_ids = train_user['question_id'].item().strip().split(',')\n",
    "                q_ids = [int(float(e.strip())) for e in q_ids]\n",
    "\n",
    "                answers = train_user['answered_correctly'].item().strip().split(',')\n",
    "                answers = [int(float(e.strip())) for e in answers]\n",
    "\n",
    "                assert len(q_ids) == len(timestamps) == len(answers)\n",
    "\n",
    "                target_index = self.get_index(timestamps, timestamp)\n",
    "                c = target_index\n",
    "                if target_index == 0:\n",
    "                    input_list = [PAD_INDEX] * ARGS.seq_size\n",
    "                else:\n",
    "                    qids = q_ids[:target_index]\n",
    "                    ans_flags = answers[:target_index]\n",
    "\n",
    "                    length = len(qids)\n",
    "                    if length > ARGS.seq_size:\n",
    "                        qids = qids[-ARGS.seq_size:]\n",
    "                        ans_flags =  ans_flags[-ARGS.seq_size:]\n",
    "                        pad_counts = 0\n",
    "                    else:\n",
    "                        pad_counts = ARGS.seq_size - length\n",
    "\n",
    "                    input_list = []\n",
    "                    for idx in range(len(qids)):\n",
    "                        tag_id = qids[idx]\n",
    "                        is_correct = ans_flags[idx]\n",
    "\n",
    "                        if is_correct:\n",
    "                            input_list.append(tag_id)\n",
    "                        else:\n",
    "                            input_list.append(tag_id + QUESTION_NUM['riii'])\n",
    "\n",
    "                    paddings = [PAD_INDEX]*pad_counts\n",
    "                    input_list = paddings + input_list\n",
    "                    assert len(input_list) == ARGS.seq_size\n",
    "\n",
    "            else:\n",
    "\n",
    "                input_list = [PAD_INDEX] * ARGS.seq_size\n",
    "\n",
    "            all_question_ids.append(input_list)\n",
    "            all_target_ids.append([target_id])\n",
    "            all_labels.append([last_is_correct])\n",
    "\n",
    "        return torch.LongTensor(all_question_ids), \\\n",
    "            torch.LongTensor(all_target_ids), \\\n",
    "            torch.LongTensor(all_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            return self.question_ids[index], self.target_ids[index]\n",
    "        else:\n",
    "            return self.question_ids[index], self.target_ids[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:18.921387Z",
     "iopub.status.busy": "2020-11-17T03:20:18.920573Z",
     "iopub.status.idle": "2020-11-17T03:20:18.924279Z",
     "shell.execute_reply": "2020-11-17T03:20:18.923515Z"
    },
    "papermill": {
     "duration": 0.041092,
     "end_time": "2020-11-17T03:20:18.924402",
     "exception": false,
     "start_time": "2020-11-17T03:20:18.883310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv = '/kaggle/input/riiid-test-answer-prediction/train.csv'\n",
    "que_csv = '/kaggle/input/riiid-test-answer-prediction/questions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:20:19.001586Z",
     "iopub.status.busy": "2020-11-17T03:20:19.000501Z",
     "iopub.status.idle": "2020-11-17T03:21:59.277758Z",
     "shell.execute_reply": "2020-11-17T03:21:59.276967Z"
    },
    "papermill": {
     "duration": 100.320562,
     "end_time": "2020-11-17T03:21:59.277882",
     "exception": false,
     "start_time": "2020-11-17T03:20:18.957320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv, usecols = [1,2,3,4,7],\n",
    "                   dtype={'timestamp':'int64',\n",
    "                         'used_id':'int16',\n",
    "                         'content_id':'int16',\n",
    "                         'content_type_id':'int8',\n",
    "                         'answered_correctly':'int8'})\n",
    "\n",
    "train_df['content_id'] = train_df['content_id'] + 1\n",
    "train_df = train_df[train_df.content_type_id == False]\n",
    "# train_df = train_df.sort_values(['timestamp'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:21:59.352454Z",
     "iopub.status.busy": "2020-11-17T03:21:59.351622Z",
     "iopub.status.idle": "2020-11-17T03:22:16.681743Z",
     "shell.execute_reply": "2020-11-17T03:22:16.681055Z"
    },
    "papermill": {
     "duration": 17.370852,
     "end_time": "2020-11-17T03:22:16.681866",
     "exception": false,
     "start_time": "2020-11-17T03:21:59.311014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1.0)\n",
    "train = train_df.iloc[:5000]\n",
    "valid = train_df.iloc[5001:5101]\n",
    "\n",
    "del(train_df)\n",
    "gc.collect()\n",
    "\n",
    "train = train.sort_values(['timestamp'], ascending=True).reset_index(drop=True)\n",
    "valid = valid.sort_values(['timestamp'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:16.758212Z",
     "iopub.status.busy": "2020-11-17T03:22:16.757170Z",
     "iopub.status.idle": "2020-11-17T03:22:16.775476Z",
     "shell.execute_reply": "2020-11-17T03:22:16.774826Z"
    },
    "papermill": {
     "duration": 0.060412,
     "end_time": "2020-11-17T03:22:16.775620",
     "exception": false,
     "start_time": "2020-11-17T03:22:16.715208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = pd.DataFrame()\n",
    "# valid = pd.DataFrame()\n",
    "    \n",
    "# for i in range(4):\n",
    "#     last_records = train_df.drop_duplicates('user_id',keep='last')\n",
    "    # 按照user_id那列的内容删除重复列，并且保存最后一个元素，也就是取每个user的最后时刻的做题记录\n",
    "#     train_df = train_df[~train_df.index.isin(last_records.index)]\n",
    "    # train.index.isin(last_recoreds.index)就是去train中检索当前index是否在last_records.index里面\n",
    "    # 如果在返回true，如果不在返回false。但是前面还有个~，所以进行了反向索引。\n",
    "#     也就是最后留下的train里面不包含last_records的内容\n",
    "#     valid = valid.append(last_records)\n",
    "            \n",
    "# for i in range(20):\n",
    "#     last_records = train_df.drop_duplicates('user_id',keep='last')\n",
    "#     train_df = train_df[~train_df.index.isin(last_records.index)]\n",
    "#     train = train.append(last_records)\n",
    "    \n",
    "    \n",
    "# del(train_df)\n",
    "# gc.collect()\n",
    "\n",
    "question_df = pd.read_csv(que_csv)\n",
    "question_df['question_id'] = question_df['question_id'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032349,
     "end_time": "2020-11-17T03:22:16.841141",
     "exception": false,
     "start_time": "2020-11-17T03:22:16.808792",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:16.913811Z",
     "iopub.status.busy": "2020-11-17T03:22:16.913033Z",
     "iopub.status.idle": "2020-11-17T03:22:17.191594Z",
     "shell.execute_reply": "2020-11-17T03:22:17.190907Z"
    },
    "papermill": {
     "duration": 0.317777,
     "end_time": "2020-11-17T03:22:17.191734",
     "exception": false,
     "start_time": "2020-11-17T03:22:16.873957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = UserSepDataSet_Rii_Train(train, question_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:17.346283Z",
     "iopub.status.busy": "2020-11-17T03:22:17.345331Z",
     "iopub.status.idle": "2020-11-17T03:22:17.350219Z",
     "shell.execute_reply": "2020-11-17T03:22:17.349501Z"
    },
    "papermill": {
     "duration": 0.124968,
     "end_time": "2020-11-17T03:22:17.350337",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.225369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:17.428305Z",
     "iopub.status.busy": "2020-11-17T03:22:17.425120Z",
     "iopub.status.idle": "2020-11-17T03:22:17.490546Z",
     "shell.execute_reply": "2020-11-17T03:22:17.489941Z"
    },
    "papermill": {
     "duration": 0.106229,
     "end_time": "2020-11-17T03:22:17.490688",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.384459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data = UserSepDataSet_Rii_Test(valid, question_df, train_data.combine, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:17.567537Z",
     "iopub.status.busy": "2020-11-17T03:22:17.566551Z",
     "iopub.status.idle": "2020-11-17T03:22:17.571076Z",
     "shell.execute_reply": "2020-11-17T03:22:17.571654Z"
    },
    "papermill": {
     "duration": 0.046955,
     "end_time": "2020-11-17T03:22:17.571806",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.524851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:17.647747Z",
     "iopub.status.busy": "2020-11-17T03:22:17.646569Z",
     "iopub.status.idle": "2020-11-17T03:22:17.652230Z",
     "shell.execute_reply": "2020-11-17T03:22:17.651443Z"
    },
    "papermill": {
     "duration": 0.045666,
     "end_time": "2020-11-17T03:22:17.652357",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.606691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03511,
     "end_time": "2020-11-17T03:22:17.722857",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.687747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SAKT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.035578,
     "end_time": "2020-11-17T03:22:17.793798",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.758220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:17.874468Z",
     "iopub.status.busy": "2020-11-17T03:22:17.873666Z",
     "iopub.status.idle": "2020-11-17T03:22:17.877176Z",
     "shell.execute_reply": "2020-11-17T03:22:17.876390Z"
    },
    "papermill": {
     "duration": 0.048365,
     "end_time": "2020-11-17T03:22:17.877307",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.828942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:17.961498Z",
     "iopub.status.busy": "2020-11-17T03:22:17.960437Z",
     "iopub.status.idle": "2020-11-17T03:22:17.963704Z",
     "shell.execute_reply": "2020-11-17T03:22:17.963071Z"
    },
    "papermill": {
     "duration": 0.050951,
     "end_time": "2020-11-17T03:22:17.963833",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.912882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:18.083210Z",
     "iopub.status.busy": "2020-11-17T03:22:18.060998Z",
     "iopub.status.idle": "2020-11-17T03:22:18.086490Z",
     "shell.execute_reply": "2020-11-17T03:22:18.085760Z"
    },
    "papermill": {
     "duration": 0.086676,
     "end_time": "2020-11-17T03:22:18.086607",
     "exception": false,
     "start_time": "2020-11-17T03:22:17.999931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model, bias=False), 4) # Q, K, V, last\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class SAKTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Encoder block of SAKT\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_head, dropout):\n",
    "        super().__init__()\n",
    "        self._self_attn = MultiHeadedAttention(num_head, hidden_dim, dropout)\n",
    "        self._ffn = PositionwiseFeedForward(hidden_dim, hidden_dim, dropout)\n",
    "        self._layernorms = clones(nn.LayerNorm(hidden_dim, eps=1e-6), 2)\n",
    "\n",
    "    def forward(self, query, key, mask=None):\n",
    "        \"\"\"\n",
    "        query: question embeddings\n",
    "        key: interaction embeddings\n",
    "        \"\"\"\n",
    "        # self-attention block\n",
    "        output = self._self_attn(query=query, key=key, value=key, mask=mask)\n",
    "        output = self._layernorms[0](key + output)\n",
    "        # feed-forward block\n",
    "        output = self._layernorms[1](output + self._ffn(output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class SAKT(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based\n",
    "    all hidden dimensions (d_k, d_v, ...) are the same as hidden_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, question_num, num_layers, num_head, dropout):\n",
    "        super().__init__()\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._question_num = question_num\n",
    "        # question_num的值是110\n",
    "        # Blocks\n",
    "        self._layers = clones(SAKTLayer(hidden_dim, num_head, dropout), num_layers)\n",
    "\n",
    "        # prediction layer\n",
    "        self._prediction = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Embedding layers\n",
    "        self._positional_embedding = nn.Embedding(ARGS.seq_size+1, hidden_dim, padding_idx=PAD_INDEX)\n",
    "        self._interaction_embedding = nn.Embedding(2*question_num+1, hidden_dim, padding_idx=PAD_INDEX)\n",
    "        # 这个就是包含了qid对错信息的矩阵\n",
    "        self._question_embedding = nn.Embedding(question_num+1, hidden_dim, padding_idx=PAD_INDEX)\n",
    "\n",
    "    def _transform_interaction_to_question_id(self, interaction):\n",
    "        \"\"\"\n",
    "        get question_id from interaction index\n",
    "        if interaction index is a number in [0, question_num], then leave it as-is\n",
    "        if interaction index is bigger than question_num (in [question_num + 1, 2 * question_num]\n",
    "        then subtract question_num\n",
    "        interaction: integer tensor of shape (batch_size, sequence_size)\n",
    "        \"\"\"\n",
    "        return interaction - self._question_num * (interaction > self._question_num).long()\n",
    "\n",
    "    def _get_position_index(self, question_id):\n",
    "        \"\"\"\n",
    "        [0, 0, 0, 4, 12] -> [0, 0, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        batch_size = question_id.shape[0]\n",
    "        position_indices = []\n",
    "        for i in range(batch_size):\n",
    "            non_padding_num = (question_id[i] != PAD_INDEX).sum(-1).item()\n",
    "            position_index = [0] * (ARGS.seq_size - non_padding_num) + list(range(1, non_padding_num+1))\n",
    "            position_indices.append(position_index)\n",
    "        return torch.tensor(position_indices, dtype=int).to(ARGS.device)\n",
    "\n",
    "    def forward(self, interaction_id, target_id):\n",
    "        \"\"\"\n",
    "        Query: Question (skill, exercise, ...) embedding\n",
    "        Key, Value: Interaction embedding + positional embedding\n",
    "        \"\"\"\n",
    "        question_id = self._transform_interaction_to_question_id(interaction_id)\n",
    "        # 这个question_id就是把以前把统一qid由于对错不同对应不同id，转换成同一qid\n",
    "        # 也就是把qid数值大于q_num的，减去q_num\n",
    "        question_id = torch.cat([question_id[:, 1:], target_id], dim=-1)\n",
    "        # question_id的原来的维度是(batch, seq_len)，\n",
    "        # 然后那两个:，第一个表示在第一个维度全选\n",
    "        # 第二个1:，表示第二个维度从第一个元素选起，这主要是为了和target_id合并在一起\n",
    "\n",
    "        # 这时question_id和interaction_id存在着一个错位的问题，\n",
    "        # 也就是question_id包含了当前target_id,而interaction_id中不包含\n",
    "\n",
    "        interaction_vector = self._interaction_embedding(interaction_id)\n",
    "        question_vector = self._question_embedding(question_id)\n",
    "\n",
    "        position_index = self._get_position_index(question_id)\n",
    "        # 对于question_id获取position的下标\n",
    "        position_vector = self._positional_embedding(position_index)\n",
    "\n",
    "        mask = get_pad_mask(question_id, PAD_INDEX) & get_subsequent_mask(question_id)\n",
    "        x = interaction_vector + position_vector\n",
    "        # 这个position_vector只加给了interaction_vector向量\n",
    "        # x的维度是(batch, seq_len, hidden)\n",
    "        for layer in self._layers:\n",
    "            x = layer(query=question_vector, key=x, mask=mask)\n",
    "\n",
    "        output = self._prediction(x)\n",
    "        # 这里的output为什么会是一个三维的向量呢\n",
    "        output = output[:, -1, :]\n",
    "        # output的最初维度是(batch, seq_len, 1)\n",
    "        # 然后用[:,-1,:]只取seq_len的最后一个值\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:18.167101Z",
     "iopub.status.busy": "2020-11-17T03:22:18.166322Z",
     "iopub.status.idle": "2020-11-17T03:22:18.173604Z",
     "shell.execute_reply": "2020-11-17T03:22:18.173001Z"
    },
    "papermill": {
     "duration": 0.051321,
     "end_time": "2020-11-17T03:22:18.173731",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.122410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ARGS.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(ARGS.random_seed)\n",
    "torch.cuda.manual_seed(ARGS.random_seed)\n",
    "torch.cuda.manual_seed_all(ARGS.random_seed)\n",
    "random.seed(ARGS.random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:18.275318Z",
     "iopub.status.busy": "2020-11-17T03:22:18.272586Z",
     "iopub.status.idle": "2020-11-17T03:22:18.368496Z",
     "shell.execute_reply": "2020-11-17T03:22:18.367868Z"
    },
    "papermill": {
     "duration": 0.143256,
     "end_time": "2020-11-17T03:22:18.368623",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.225367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " model = SAKT(ARGS.hidden_dim, QUESTION_NUM['riii'], ARGS.num_layers,\n",
    "                     ARGS.num_head, ARGS.dropout).to(ARGS.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.045479,
     "end_time": "2020-11-17T03:22:18.461095",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.415616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035732,
     "end_time": "2020-11-17T03:22:18.533086",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.497354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:18.613085Z",
     "iopub.status.busy": "2020-11-17T03:22:18.612037Z",
     "iopub.status.idle": "2020-11-17T03:22:18.615469Z",
     "shell.execute_reply": "2020-11-17T03:22:18.614727Z"
    },
    "papermill": {
     "duration": 0.046595,
     "end_time": "2020-11-17T03:22:18.615591",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.568996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "train_dataloader = data.DataLoader(\n",
    "            dataset=train_data, shuffle=True,\n",
    "            batch_size=ARGS.train_batch, num_workers=ARGS.num_workers)\n",
    "valid_dataloader = data.DataLoader(\n",
    "            dataset=val_data, shuffle=False,\n",
    "            batch_size=ARGS.train_batch, num_workers=ARGS.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.035959,
     "end_time": "2020-11-17T03:22:18.687775",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.651816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:18.783509Z",
     "iopub.status.busy": "2020-11-17T03:22:18.782613Z",
     "iopub.status.idle": "2020-11-17T03:22:18.786263Z",
     "shell.execute_reply": "2020-11-17T03:22:18.785424Z"
    },
    "papermill": {
     "duration": 0.062342,
     "end_time": "2020-11-17T03:22:18.786398",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.724056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps\n",
    "        ])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "               (self.model_size ** (-0.5) *\n",
    "                min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:18.880229Z",
     "iopub.status.busy": "2020-11-17T03:22:18.874854Z",
     "iopub.status.idle": "2020-11-17T03:22:18.921344Z",
     "shell.execute_reply": "2020-11-17T03:22:18.920631Z"
    },
    "papermill": {
     "duration": 0.097859,
     "end_time": "2020-11-17T03:22:18.921465",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.823606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoamOptimizer:\n",
    "    def __init__(self, model, lr, model_size, warmup):\n",
    "        self._adam = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self._opt = NoamOpt(\n",
    "            model_size=model_size, factor=1, warmup=warmup, optimizer=self._adam)\n",
    "\n",
    "    def step(self, loss):\n",
    "        self._opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self._opt.step()\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, device, warm_up_step_count,\n",
    "                 d_model, num_epochs, weight_path, lr,\n",
    "                 train_data, val_data, test_data=None):\n",
    "        self._device = device\n",
    "        self._num_epochs = num_epochs\n",
    "        self._weight_path = weight_path\n",
    "\n",
    "        self._model = model\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self._model.to(device)\n",
    "\n",
    "        self._train_data = train_data\n",
    "        self._val_data = val_data\n",
    "        self._test_data = test_data\n",
    "\n",
    "        self._opt = NoamOptimizer(model=model, lr=lr, model_size=d_model, warmup=warm_up_step_count)\n",
    "\n",
    "        self.step = 0\n",
    "        self._threshold = 0.5\n",
    "        self.max_step = 0\n",
    "        self.max_acc = 0.0\n",
    "        self.max_auc = 0.0\n",
    "\n",
    "        self.test_acc = 0.0\n",
    "        self.test_auc = 0.0\n",
    "\n",
    "    # train model and choose weight with max auc on validation dataset\n",
    "    def train(self):\n",
    "        train_gen = self._train_data\n",
    "        val_gen = self._val_data\n",
    "\n",
    "        # will train self._num_epochs copies of train data\n",
    "        to_train = chain.from_iterable(repeat(train_gen, self._num_epochs))\n",
    "        # consisting of total_steps batches\n",
    "        total_steps = len(train_gen) * self._num_epochs\n",
    "        print(\"total_steps:\",total_steps)\n",
    "\n",
    "        self.step = 0\n",
    "        while self.step < total_steps:\n",
    "            rem_steps = total_steps - self.step\n",
    "            num_steps = min(rem_steps, ARGS.train_steps)\n",
    "            self.step += num_steps\n",
    "\n",
    "            # take num_steps batches from to_train stream\n",
    "            train_batches = islice(to_train, num_steps)\n",
    "            # print(f'Step: {self.step}')\n",
    "#            print(f\"Current Training step is: {self.step}\")\n",
    "            self._train(train_batches, num_steps)\n",
    "            if self.step % ARGS.eval_steps == 0:\n",
    "                cur_weight = self._model.state_dict()\n",
    "#                 torch.save(cur_weight, f'{self._weight_path}{self.step}.pt')\n",
    "                self._test('Validation', val_gen)\n",
    "            # print(f'Current best weight: {self.max_step}.pt, best auc: {self.max_auc:.4f}')\n",
    "            # remove all weight file except {self.max_step}.pt\n",
    "                print(f\"Validation-- Best validaction acc is: {self.max_acc:.4f},\"\n",
    "                  f\"Best auc is:{self.max_auc:.4f}.\\n\")\n",
    "#             weight_list = os.listdir(self._weight_path)\n",
    "            # for w in weight_list:\n",
    "            #     if int(w[:-3]) != self.max_step:\n",
    "            #         os.unlink(f'{self._weight_path}{w}')\n",
    "        self._test('Validation', val_gen)\n",
    "    # get test results\n",
    "    def test(self, weight_num):\n",
    "        test_gen = data.DataLoader(\n",
    "            dataset=self._test_data, shuffle=False,\n",
    "            batch_size=ARGS.test_batch, num_workers=ARGS.num_workers)\n",
    "\n",
    "        # load best weight\n",
    "        if self.max_step != 0:\n",
    "            weight_num = self.max_step\n",
    "        weight_path = f'{ARGS.weight_path}{weight_num}.pt'\n",
    "        print(f'best weight: {weight_path}')\n",
    "        self._model.load_state_dict(torch.load(weight_path))\n",
    "        self._test('Test', test_gen)\n",
    "\n",
    "    def _forward(self, batch):\n",
    "#        batch = {k: t.to(self._device) for k, t in batch.items()}\n",
    "#        label = batch['label']  # shape: (batch_size, 1)\n",
    "\n",
    "#        output = self._model(batch['input'], batch['target_id'])\n",
    "\n",
    "        batch = tuple(t.to(self._device) for t in batch)\n",
    "        question_id, target_id, label = batch\n",
    "\n",
    "        output = self._model(question_id, target_id)\n",
    "        pred = (torch.sigmoid(output) >= self._threshold).long()  # shape: (batch_size, 1)\n",
    "        # 感觉这里的sigmoid加在output之前是不是会更好\n",
    "\n",
    "        return label, output, pred\n",
    "\n",
    "    def _get_loss(self, label, output):\n",
    "        # 这里的label我可以理解为是一个[0,1]序列，但是output却是上面_forward的output，不是一个（0，1）范围的值\n",
    "        # 是因为loss_fn里面有sigmoid\n",
    "        loss = self._loss_fn(output, label.float())\n",
    "        return loss.mean()\n",
    "\n",
    "    # takes iterator\n",
    "    def _train(self, batch_iter, num_batches):\n",
    "        start_time = time.time()\n",
    "        self._model.train()\n",
    "\n",
    "        losses = []\n",
    "        num_corrects = 0\n",
    "        num_total = 0\n",
    "        labels = []\n",
    "        outs = []\n",
    "\n",
    "        for batch in tqdm(batch_iter, total=num_batches):\n",
    "            label, out, pred = self._forward(batch)\n",
    "            train_loss = self._get_loss(label, out)\n",
    "            losses.append(train_loss.item())\n",
    "\n",
    "            self._opt.step(train_loss)\n",
    "\n",
    "            num_corrects += (pred == label).sum().item()\n",
    "            num_total += len(label)\n",
    "\n",
    "            labels.extend(label.squeeze(-1).data.cpu().numpy())\n",
    "            outs.extend(out.squeeze(-1).data.cpu().numpy())\n",
    "\n",
    "        acc = num_corrects / num_total\n",
    "        auc = roc_auc_score(labels, outs)\n",
    "        loss = np.mean(losses)\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        if self.step % ARGS.train_steps:\n",
    "            print(f\"Current Training step is: {self.step}\")\n",
    "            print(f'Training correct predict num is: {num_corrects}, total num is: {num_total}')\n",
    "            print(f'[Training]     time: {training_time:.2f}, loss: {loss:.4f}, acc: {acc:.4f}, auc: {auc:.4f}')\n",
    "\n",
    "    # takes iterable\n",
    "    def _test(self, name, batches):\n",
    "        start_time = time.time()\n",
    "        self._model.eval()\n",
    "\n",
    "        losses = []\n",
    "        num_corrects = 0\n",
    "        num_total = 0\n",
    "        labels = []\n",
    "        outs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(batches):\n",
    "                label, out, pred = self._forward(batch)\n",
    "                test_loss = self._get_loss(label, out)\n",
    "                losses.append(test_loss.item())\n",
    "\n",
    "                num_corrects += (pred == label).sum().item()\n",
    "                num_total += len(label)\n",
    "\n",
    "                labels.extend(label.squeeze(-1).data.cpu().numpy())\n",
    "                outs.extend(out.squeeze(-1).data.cpu().numpy())\n",
    "\n",
    "        acc = num_corrects / num_total\n",
    "        auc = roc_auc_score(labels, outs)\n",
    "        loss = np.mean(losses)\n",
    "        # training_time = time.time() - start_time\n",
    "\n",
    "        print(f'correct predict num is: {num_corrects}, total num is: {num_total}')\n",
    "        print(f' loss is: {loss:.4f}, acc: {acc:.4f}, auc: {auc:.4f}')\n",
    "\n",
    "        if name == 'Validation':\n",
    "            if self.max_auc < auc:\n",
    "                self.max_auc = auc\n",
    "                self.max_acc = acc\n",
    "                self.max_step = self.step\n",
    "                torch.save(self._model.state_dict(), self._weight_path)\n",
    "\n",
    "        elif name == 'Test':\n",
    "            self.test_acc = acc\n",
    "            self.test_auc = auc\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:19.002649Z",
     "iopub.status.busy": "2020-11-17T03:22:19.001593Z",
     "iopub.status.idle": "2020-11-17T03:22:19.006093Z",
     "shell.execute_reply": "2020-11-17T03:22:19.005314Z"
    },
    "papermill": {
     "duration": 0.047427,
     "end_time": "2020-11-17T03:22:19.006220",
     "exception": false,
     "start_time": "2020-11-17T03:22:18.958793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model,ARGS.device, ARGS.warm_up_step_count,\n",
    "                 ARGS.hidden_dim, ARGS.num_epochs, 'rid_model.pt',\n",
    "                 ARGS.lr, train_dataloader, valid_dataloader, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:19.087027Z",
     "iopub.status.busy": "2020-11-17T03:22:19.086240Z",
     "iopub.status.idle": "2020-11-17T03:22:21.254121Z",
     "shell.execute_reply": "2020-11-17T03:22:21.253178Z"
    },
    "papermill": {
     "duration": 2.210901,
     "end_time": "2020-11-17T03:22:21.254297",
     "exception": false,
     "start_time": "2020-11-17T03:22:19.043396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.12it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Training step is: 10\n",
      "Training correct predict num is: 1885, total num is: 5000\n",
      "[Training]     time: 1.97, loss: 0.7199, acc: 0.3770, auc: 0.4981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct predict num is: 39, total num is: 100\n",
      " loss is: 0.7133, acc: 0.3900, auc: 0.5310\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.062209,
     "end_time": "2020-11-17T03:22:21.375572",
     "exception": false,
     "start_time": "2020-11-17T03:22:21.313363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043873,
     "end_time": "2020-11-17T03:22:21.464276",
     "exception": false,
     "start_time": "2020-11-17T03:22:21.420403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 模型解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:21.561394Z",
     "iopub.status.busy": "2020-11-17T03:22:21.560269Z",
     "iopub.status.idle": "2020-11-17T03:22:21.615255Z",
     "shell.execute_reply": "2020-11-17T03:22:21.614631Z"
    },
    "papermill": {
     "duration": 0.106969,
     "end_time": "2020-11-17T03:22:21.615385",
     "exception": false,
     "start_time": "2020-11-17T03:22:21.508416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " model = SAKT(ARGS.hidden_dim, QUESTION_NUM['riii'], ARGS.num_layers,\n",
    "                     ARGS.num_head, ARGS.dropout).to(ARGS.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:21.710840Z",
     "iopub.status.busy": "2020-11-17T03:22:21.709533Z",
     "iopub.status.idle": "2020-11-17T03:22:21.726535Z",
     "shell.execute_reply": "2020-11-17T03:22:21.725689Z"
    },
    "papermill": {
     "duration": 0.066471,
     "end_time": "2020-11-17T03:22:21.726664",
     "exception": false,
     "start_time": "2020-11-17T03:22:21.660193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAKT(\n",
       "  (_layers): ModuleList(\n",
       "    (0): SAKTLayer(\n",
       "      (_self_attn): MultiHeadedAttention(\n",
       "        (linears): ModuleList(\n",
       "          (0): Linear(in_features=100, out_features=100, bias=False)\n",
       "          (1): Linear(in_features=100, out_features=100, bias=False)\n",
       "          (2): Linear(in_features=100, out_features=100, bias=False)\n",
       "          (3): Linear(in_features=100, out_features=100, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (_ffn): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (w_2): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (_layernorms): ModuleList(\n",
       "        (0): LayerNorm((100,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): LayerNorm((100,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_prediction): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (_positional_embedding): Embedding(21, 100, padding_idx=0)\n",
       "  (_interaction_embedding): Embedding(27047, 100, padding_idx=0)\n",
       "  (_question_embedding): Embedding(13524, 100, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('rid_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:21.840394Z",
     "iopub.status.busy": "2020-11-17T03:22:21.839364Z",
     "iopub.status.idle": "2020-11-17T03:22:21.843114Z",
     "shell.execute_reply": "2020-11-17T03:22:21.842405Z"
    },
    "papermill": {
     "duration": 0.054468,
     "end_time": "2020-11-17T03:22:21.843248",
     "exception": false,
     "start_time": "2020-11-17T03:22:21.788780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iter_test = env.iter_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:21.953515Z",
     "iopub.status.busy": "2020-11-17T03:22:21.948317Z",
     "iopub.status.idle": "2020-11-17T03:22:23.117543Z",
     "shell.execute_reply": "2020-11-17T03:22:23.118112Z"
    },
    "papermill": {
     "duration": 1.225794,
     "end_time": "2020-11-17T03:22:23.118277",
     "exception": false,
     "start_time": "2020-11-17T03:22:21.892483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for(test_df, sample_prediction_df) in iter_test:\n",
    "    test_df['content_id'] = test_df['content_id'] + 1\n",
    "    test_df = pd.merge(test_df, question_df, left_on='content_id', \n",
    "                       right_on='question_id', how='left')\n",
    "    test_df = test_df.loc[:,['content_id','content_type_id',\n",
    "                             'row_id','user_id','timestamp']]\n",
    "#     test_df.drop(['content_id'], axis=1)\n",
    "#     test_df['answered_correctly_user'].fillna(0.5, inplace=True)\n",
    "#     test_df['answered_correctly_content'].fillna(0.5, inplace=True)\n",
    "#     test_df.drop(['part','prior_question_elapsed_time','prior_question_had_explanation'],axis=1)\n",
    "#     test_df['part'].fillna(4, inplace=True)\n",
    "#     test_df['prior_question_elapsed_time'].fillna(0.0, inplace = True)\n",
    "#     test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "#     test_df.fillna(0.0,inplace=True)\n",
    "#     question_df.fillna(0.0,inplace=True)\n",
    "#     train_data.combine.fillna(0.0,inplace=True)\n",
    "    test_df = test_df.loc[test_df['content_type_id'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    test_dataset = UserSepDataSet_Rii_Test(test_df, question_df, train_data.combine, is_test=True)\n",
    "    test_dataloader = data.DataLoader(\n",
    "            dataset=test_dataset, shuffle=False,\n",
    "            batch_size=ARGS.train_batch, num_workers=ARGS.num_workers)\n",
    "    model =  SAKT(ARGS.hidden_dim, QUESTION_NUM['riii'], ARGS.num_layers,\n",
    "                     ARGS.num_head, ARGS.dropout).to(ARGS.device)\n",
    "    model.load_state_dict(torch.load('rid_model.pt'))\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            batch = tuple(t.to(ARGS.device) for t in batch)\n",
    "            question_id, target_id = batch\n",
    "            output = model(question_id, target_id)\n",
    "            pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "            pred = pred.view(-1)\n",
    "            preds.extend(pred)\n",
    "        preds = [int(e) for e in preds]\n",
    "        test_df['answered_correctly'] =  preds\n",
    "        env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T03:22:23.228868Z",
     "iopub.status.busy": "2020-11-17T03:22:23.227717Z",
     "iopub.status.idle": "2020-11-17T03:22:23.232339Z",
     "shell.execute_reply": "2020-11-17T03:22:23.231692Z"
    },
    "papermill": {
     "duration": 0.064295,
     "end_time": "2020-11-17T03:22:23.232469",
     "exception": false,
     "start_time": "2020-11-17T03:22:23.168174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"./submission.csv\")\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.050358,
     "end_time": "2020-11-17T03:22:23.333824",
     "exception": false,
     "start_time": "2020-11-17T03:22:23.283466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "papermill": {
   "duration": 132.783786,
   "end_time": "2020-11-17T03:22:23.492755",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-17T03:20:10.708969",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
