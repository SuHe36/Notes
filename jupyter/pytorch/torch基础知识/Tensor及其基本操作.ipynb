{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch是一个基于python的科学计算包，主要定位两种人群\n",
    "#1、numpy的替代品，可以利用GPU的性能进行计算。\n",
    "#2、深度学习研究平台拥有足够的灵活性和速度\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -8.5899e+09,  0.0000e+00],\n",
      "        [-8.5899e+09,  1.4569e-19,  2.7517e+12],\n",
      "        [ 7.5338e+28,  3.0313e+32,  6.3828e+28],\n",
      "        [ 1.4603e-19,  1.0899e+27,  6.8943e+34],\n",
      "        [ 1.1835e+22,  7.0976e+22,  1.8515e+28]])\n",
      "tensor([[0.4467, 0.8242, 0.6860],\n",
      "        [0.7444, 0.7867, 0.0798],\n",
      "        [0.4781, 0.0286, 0.8030],\n",
      "        [0.3589, 0.8433, 0.4196],\n",
      "        [0.9569, 0.0645, 0.8398]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 0.8580,  0.6247,  0.0648],\n",
      "        [ 0.1715, -1.2764,  1.0940],\n",
      "        [-0.4517,  0.9479,  1.7505],\n",
      "        [ 0.1424,  0.2202, -0.7999],\n",
      "        [-1.9016, -1.1691, -0.5384]])\n"
     ]
    }
   ],
   "source": [
    "#张量Tensor\n",
    "#Tensor类似于numpy中的ndarrays,同时tensors可以使用GPU进行计算\n",
    "import torch\n",
    "\n",
    "x = torch.empty(5,3)\n",
    "#torch.empty()可以不初始化的构建矩阵\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(5,3)\n",
    "#torch.rand()可以随机初始化一个矩阵\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(5,3, dtype=torch.long)\n",
    "#torch.zeros()可以构建一个全是0的矩阵\n",
    "print(x)\n",
    "\n",
    "x = torch.tensor([5.5,3])\n",
    "#torch.tensor()可以直接使用数据构建一个张量\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(5,3)\n",
    "#torch.ones()可以创建全是1的矩阵\n",
    "print(x)\n",
    "\n",
    "y = x.new_ones(5,3)\n",
    "#tensor.new_ones()可以继承原tensor的数据类型dataType，来生成新的tensor\n",
    "#tensor.ones()是直接生成新的tensor\n",
    "print(y)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3437, -0.1685, -1.1374],\n",
      "        [ 0.2528, -0.2483,  0.8504],\n",
      "        [-1.3674,  1.8896, -0.2811],\n",
      "        [-0.5810,  0.0848,  0.0230],\n",
      "        [ 2.1306,  0.9101, -0.6635]])\n",
      "tensor([[0.4810, 0.8788, 0.9358],\n",
      "        [0.2410, 0.8808, 0.6105],\n",
      "        [0.7998, 0.7386, 0.2780],\n",
      "        [0.1035, 0.8543, 0.7332],\n",
      "        [0.8008, 0.8949, 0.3246]])\n"
     ]
    }
   ],
   "source": [
    "#torch.rand()和torch.randn()的区别是什么\n",
    "#一个是均匀分布，一个是正态分布\n",
    "\n",
    "#torch.rand(*sizes, out=None)返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，张量的形状由参数sizes定义\n",
    "\n",
    "#torch.randn(*sizes, out=None)返回一个张量，包含了从正态分布(均值为0，方差为1，即高斯分布)中抽取的一组随机数，张量的形状由sizes决定\n",
    "\n",
    "\n",
    "x = torch.randn(5,3)\n",
    "print(x)\n",
    "\n",
    "y = torch.rand(5,3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor attributes\n",
    "#在tensor attributes中有三个类，分别为torch.dtype、torch.device和torch.layout。\n",
    "#其中torch.dtype是展示torch.Tensor数据类型的类，pytorch有八个不同的数据类型，下表是完整的dtype列表\n",
    "#1: 32-bit floating point -> torch.float32 or torch.float\n",
    "#2: 64-bit floating point -> torch.float64 or torch.double\n",
    "#3: 16-bit floating point -> torch.float16 or torch.half\n",
    "#4: 8-bit integer(unsigned) -> torch.uint8\n",
    "#5: 8-bit integer(signed) -> torch.int8\n",
    "#6: 16-bit integer(signed) -> torch.int16 or troch.short\n",
    "#7: 32-bit integer(signed) -> torch.int32 or torch.int\n",
    "#8: 64-bit integer(signed) -> torch.int64 or torch.long\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Torch.device是表现torch.tensor被分配的设备类型的类，其中分为”cpu“和”cuda“两种\n",
    "#如果设备序号没有显示则表示此tensor被分配到当前设备，\n",
    "#比如:\"cuda\"等同于\"cuda\":X, X为torch.cuda.current_device()返回值\n",
    "#我们可以通过tensor.device来获取其属性，同时可以利用字符或者字符+序号的方式来分配设备\n",
    "\n",
    "#通过字符串\n",
    "torch.device(\"cuda:0\")\n",
    "torch.device(\"cpu\")\n",
    "torch.device(\"cuda\")#当前设备\n",
    "\n",
    "\n",
    "#通过字符串和设备序号\n",
    "torch.device(\"cuda\",0)\n",
    "torch.device(\"cpu\",0)\n",
    "\n",
    "\n",
    "#此外，cpu和cuda设备的转换可以通过使用”to“来实现\n",
    "device_cpu = torch.device(\"cpu\") #声明cpu设备\n",
    "device_cuda = torch.device(\"cuda\") #声明cuda设备\n",
    "data = torch.Tensor([1])\n",
    "data.to(device_cpu)#将数据转换为cpu格式\n",
    "#data.to(device_cuda)#将数据转换为cuda格式，这里我们执行会报错，是因为没装cuda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "[1 2 3]\n",
      "[-1  2  3]\n"
     ]
    }
   ],
   "source": [
    "#创建Tensor\n",
    "\n",
    "#直接创建:torch.tensor(data, dtype=None, device=None, requires_grad=False)\n",
    "#data--可以是list,tuple,numpy array, scalar等类型数据\n",
    "#dtype--可以返回想要的tensor类型\n",
    "#device--可以指定返回的设备\n",
    "#requires_grad--可以指定是否进行记录图的操作，默认为false\n",
    "\n",
    "#需要注意的是，torch.tensor()总是复制data,如果你想要避免复制，可以使用torch.Tensor.detach()\n",
    "#如果是从numpy中获得的数据，那么可以使用torch.from_numpy()，注from_numpy()是共享内存的\n",
    "torch.tensor([[0.1,1.2],[2.2,3.1],[4.9,5.2]])\n",
    "torch.tensor([0,1])\n",
    "torch.tensor([[0.11,0.22,0.33]], dtype=torch.float64, device=torch.device('cpu',0))\n",
    "#torch.tensor([0.11,0.22,0.33], dtype=torch.float64, device=torch.device('cpu',0))\n",
    "torch.tensor(3.1411) #create a scalar(zero-dimensional tensor)\n",
    "torch.tensor([])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "[1 2 3]\n",
      "[-1  2  3]\n"
     ]
    }
   ],
   "source": [
    "#从numpy中获得数据：torch.from_numpy(ndarry)\n",
    "#注：生成返回的tensor会和ndarry共享数据，任何对tensor的操作都会影响到ndarry。\n",
    "\n",
    "import numpy\n",
    "a = numpy.array([1,2,3])\n",
    "t = torch.from_numpy(a)\n",
    "print(t)\n",
    "print(a)\n",
    "t[0] = -1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0404, 1.0808, 1.1212, 1.1616, 1.2020, 1.2424, 1.2828, 1.3232,\n",
       "        1.3636, 1.4040, 1.4444, 1.4848, 1.5253, 1.5657, 1.6061, 1.6465, 1.6869,\n",
       "        1.7273, 1.7677, 1.8081, 1.8485, 1.8889, 1.9293, 1.9697, 2.0101, 2.0505,\n",
       "        2.0909, 2.1313, 2.1717, 2.2121, 2.2525, 2.2929, 2.3333, 2.3737, 2.4141,\n",
       "        2.4545, 2.4949, 2.5354, 2.5758, 2.6162, 2.6566, 2.6970, 2.7374, 2.7778,\n",
       "        2.8182, 2.8586, 2.8990, 2.9394, 2.9798, 3.0202, 3.0606, 3.1010, 3.1414,\n",
       "        3.1818, 3.2222, 3.2626, 3.3030, 3.3434, 3.3838, 3.4242, 3.4646, 3.5051,\n",
       "        3.5455, 3.5859, 3.6263, 3.6667, 3.7071, 3.7475, 3.7879, 3.8283, 3.8687,\n",
       "        3.9091, 3.9495, 3.9899, 4.0303, 4.0707, 4.1111, 4.1515, 4.1919, 4.2323,\n",
       "        4.2727, 4.3131, 4.3535, 4.3939, 4.4343, 4.4747, 4.5152, 4.5556, 4.5960,\n",
       "        4.6364, 4.6768, 4.7172, 4.7576, 4.7980, 4.8384, 4.8788, 4.9192, 4.9596,\n",
       "        5.0000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建特定的tensor\n",
    "\n",
    "#根据数值要求\n",
    "\n",
    "#torch.zeros(*size, out=None)返回大小为size的零矩阵\n",
    "torch.zeros([2,2])\n",
    "torch.zeros((2,2))\n",
    "#上面的两种方式都生成了一个2*2的矩阵\n",
    "a = torch.zeros(8) #生成一个长度为8的一维零矩阵\n",
    "\n",
    "#torch.zeros_like(input)，返回与input相同size的零矩阵\n",
    "torch.zeros_like(a)\n",
    "\n",
    "#torch.ones(*sizes, out=None)返回大小为sizes的单位矩阵\n",
    "#torch.ones_like(input),返回与input相同size的单位矩阵\n",
    "torch.ones((3,4)) #等同于torch.ones([3,4])，后面的方法也是一样的\n",
    "\n",
    "#torch.full(size, fill_value),返回大小为size,单位值为fill_value的矩阵\n",
    "#torch.full_like(input, fill_value)，返回与input相同size，单位值为fill_value的矩阵\n",
    "torch.full([2,3],4)\n",
    "\n",
    "#torch.arange(star=0, end, step=1,...)，返回从start到end，单位步长为step的1d tensor\n",
    "#也就是生成一个等差数列，起始值为start,末尾值为end-step，间隔为step\n",
    "torch.arange(1,4,0.5)\n",
    "torch.arange(1,6)#输出结果为tensor([1, 2, 3, 4, 5])，step默认为1，但不包括最后一个元素\n",
    "torch.arange(start=1, end=4, step=0.5)\n",
    "\n",
    "#torch.linspace(start, end, steps=100),返回从start到end，间隔中的插值数目为steps的1d tensor\n",
    "#也是生成一个等差数列，起始值为start,末尾值为end，中间要有steps个元素值\n",
    "torch.linspace(1,5,100)\n",
    "\n",
    "#torch.logspace(start, end, steps=100,)返回从10^start到10^end的有steps个间隔的1d tensor\n",
    "#这个具体怎么生成的没看懂，但是每一个元素都是前一个元素的平方值\n",
    "torch.logspace(1,5,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "#创建特定的tensor\n",
    "\n",
    "#根据矩阵的要求\n",
    "\n",
    "#torch.eye(n, m=None, out=None...)，返回2D的单位对角矩阵\n",
    "#默认的生成的是一个n*n的矩阵，如果强制给m赋值也可以\n",
    "torch.eye(8)\n",
    "torch.eye(n=8,m=4)\n",
    "torch.eye(8,4)\n",
    "\n",
    "#torch.empty(*size, out=None,...)返回被未初始化的数值填充，大小为sizes的tensor\n",
    "#torch.empty_like(input,...)返回与input相同size，并被未初始化的数值填充的tensor\n",
    "torch.empty(8)\n",
    "torch.empty((2,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 0, 2, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建特定的tensor\n",
    "import torch\n",
    "#随机采样生成\n",
    "\n",
    "#torch.normal(mean, std, out=None)\n",
    "#返回的是一个张量，包含从给定参数Means，std的离散正态分布中抽取随机数。\n",
    "#均值means是一个张量，包含每个输出元素相关的正态分布的均值\n",
    "#标准差值std是一个张量，包含每个输出元素相关的正态分布的标准差\n",
    "#均值和标准差的形状不需要匹配，但每个张量的元素个数必须相同。\n",
    "#参数：means(Tensor) -- 均值，也可以是一个值，表示后面的std几个张量共用一个mean值\n",
    "#     std(Tensor) -- 标准差, 也可以是一个值，表示前面的mean几个张量共用一个std值\n",
    "#     out(Tensor) --可选的输出张量\n",
    "torch.normal(mean=torch.rand(5), std=torch.linspace(1,2,5))\n",
    "torch.normal(mean=1, std=torch.linspace(1,2,5))\n",
    "torch.normal(mean=torch.rand(5), std=2)\n",
    "\n",
    "\n",
    "#torch.rand(*size, out=None, dtype=None)返回size个[0,1]之间均匀分布的随机数值\n",
    "#troch.rand_like(input, dtype=None,...)返回与input相同size的tensor，填充均匀分布的随机数值\n",
    "a = torch.rand(5)\n",
    "torch.rand_like(a)\n",
    "\n",
    "\n",
    "#torch.randint(low=0, high,size...)返回均匀分布的[low,high]之间的整数随机值,注意size需要是一个元组\n",
    "#torch.randint_like返回与input相同size的tensor，填充均匀分布的随机数值\n",
    "a = torch.randint(low=1, high=5,size=(3,))\n",
    "torch.randint(1,5,(4,))\n",
    "torch.randint_like(a, 2,4)\n",
    "\n",
    "\n",
    "#torch.randn(*size, out=None,...)，返回大小为size，均值为0，方差为1的正太分布的随机数值\n",
    "#torch.randn_like(input,dtype=None,...)\n",
    "a = torch.randn(5)\n",
    "torch.randn_like(a)\n",
    "\n",
    "\n",
    "#torch.randperm(n, out=None, dtype=torch.int64)，返回0到n-1的数列的随机排列\n",
    "torch.randperm(5)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 1., 2., 3.])\n",
      "tensor([[0.1671, 0.8666, 0.8820, 0.2945],\n",
      "        [0.5202, 0.6667, 0.3512, 0.5016]])\n",
      "tensor([[-1.0040, -0.6113, -0.9164,  0.4221],\n",
      "        [-0.7211, -1.2085,  0.2746,  0.4474]])\n",
      "tensor([[-1.0040, -0.6113, -0.9164,  0.4221,  0.1671,  0.8666,  0.8820,  0.2945],\n",
      "        [-0.7211, -1.2085,  0.2746,  0.4474,  0.5202,  0.6667,  0.3512,  0.5016]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[[ 9, 10],\n",
      "         [11, 12]],\n",
      "\n",
      "        [[13, 14],\n",
      "         [15, 16]]])\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]],\n",
      "\n",
      "        [[13, 14],\n",
      "         [15, 16]]])\n",
      "torch.Size([4, 2, 2])\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4],\n",
      "         [ 9, 10],\n",
      "         [11, 12]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8],\n",
      "         [13, 14],\n",
      "         [15, 16]]])\n",
      "torch.Size([2, 4, 2])\n",
      "tensor([[[[ 1,  2],\n",
      "          [ 3,  4]],\n",
      "\n",
      "         [[ 5,  6],\n",
      "          [ 7,  8]]],\n",
      "\n",
      "\n",
      "        [[[ 9, 10],\n",
      "          [11, 12]],\n",
      "\n",
      "         [[13, 14],\n",
      "          [15, 16]]]])\n",
      "torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "#Tensor的基本操作\n",
    "\n",
    "#torch.cat(seq,dim=0, out=None),沿着dim连接seq中的tensor，所有的tensor必须有相同的size或者empty\n",
    "#其相反的操作为torch.split()和torch.chunk()\n",
    "#torch.stack(seq)的操作是直接再新增加一个维度，cat的操作其实是在原有的维度里面修改数值，\n",
    "#stackl和cat的具体区别可以看下面的g和i的size区别\n",
    "\n",
    "a = torch.Tensor([1,2,3])\n",
    "b = torch.cat((a,a))\n",
    "print(b)\n",
    "b = torch.randn(8).reshape((2,4))\n",
    "c = torch.rand(8).reshape((2,4))\n",
    "d = torch.cat((b,c),dim=1)\n",
    "print(c)\n",
    "print(b)\n",
    "print(d)\n",
    "\n",
    "e = torch.arange(1,9).reshape(2,2,2)\n",
    "f = torch.arange(9,17).reshape(2,2,2)\n",
    "g = torch.cat((e,f))\n",
    "h = torch.cat((e,f),dim=1)\n",
    "i = torch.stack((e,f))\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(g.size())\n",
    "print(h)\n",
    "print(h.size())\n",
    "print(i)\n",
    "print(i.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2.]), tensor([3.]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.gather(input, dim, index, out=None)返回沿着dim搜集的新的tensor\n",
    "#对于3D的张量来说，可以视为：\n",
    "#if dim=0: out[i][j][k] = input[index[i][j][k]][j][k]\n",
    "#if dim=1: out[i][j][k] = input[i][index[i][j][k]][k]\n",
    "#if dim=2: out[i][j][k] = input[i][j][index[i][j][k]]\n",
    "\n",
    "t = torch.Tensor([[1,2],[3,4]])\n",
    "ind = torch.LongTensor([[0,0],[1,0]])\n",
    "torch.gather(t,dim=0,index=ind)\n",
    "\n",
    "#torch.split(tensor, split_size, dim),将tensor沿着dim维切分成split_size大小的tensor\n",
    "#torch.chunk(tensor,chunks,dim)，将tensor沿着dim为切分成chunks组，如果不能整除的话，最后一块可以小一些\n",
    "a = torch.Tensor([1,2,3])\n",
    "torch.split(a,1)#输出结果为(tensor([1.]), tensor([2.]), tensor([3.]))，每组一个tensor\n",
    "torch.chunk(a,1)#输出结果为(tensor([1., 2., 3.]),)切分成了1组\n",
    "torch.chunk(a,2)#输出结果为(tensor([1., 2.]), tensor([3.]))，切分成了2组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-0.1488, -0.3783, -1.2725,  0.2263],\n",
      "        [-0.0348,  0.2564,  1.5160,  0.6048],\n",
      "        [-0.2523, -0.1958,  0.3398, -0.5247]])\n",
      "tensor([[-0.1488, -0.3783, -1.2725,  0.2263],\n",
      "        [-0.2523, -0.1958,  0.3398, -0.5247]])\n",
      "tensor([[-0.1488, -1.2725],\n",
      "        [-0.0348,  1.5160],\n",
      "        [-0.2523,  0.3398]])\n",
      "x tensor([[-1.9372, -0.0672,  0.7725,  1.2513],\n",
      "        [-1.9247, -0.2642,  0.4124, -0.6706],\n",
      "        [-0.0664, -0.3396,  0.7775, -0.9599]])\n",
      "mask tensor([[0, 0, 1, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.uint8)\n",
      "tensor([0.7725, 1.2513, 0.7775])\n"
     ]
    }
   ],
   "source": [
    "#torch.index_select(input, dim, index, out=None),\n",
    "#在指定维度dim方向上从input中抽取由位置序列Index指定的值，\n",
    "#output的其他维度的长度和原来的矩阵相同，在第dim维度上的长度和index的长度相同。\n",
    "\n",
    "\n",
    "x = torch.randn(3,4)\n",
    "print(\"x:\",x)\n",
    "indices = torch.LongTensor([0,2])\n",
    "print(torch.index_select(x,0,indices))#dim=0\n",
    "print(torch.index_select(x,1,indices))#dim=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#torch.mask_select(input, mask, out=None),\n",
    "#mask是一个ByteTensor，维度与input相同，true返回，false不返回，返回值不共用内存\n",
    "#返回的结果为一个1D的Tensor\n",
    "x = torch.randn(3,4)\n",
    "print(\"x\",x)\n",
    "mask = x.ge(0.5)\n",
    "print(\"mask\",mask)\n",
    "print(torch.masked_select(x,mask))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([[1, 2, 3, 4]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#矩阵的一些操作\n",
    "#torch.transpose(input, dim0, dim1, out=None) 返回dim0和dim1交换后的tensor\n",
    "#torch.t(input, out=None)，专为2D矩阵的转置，是transpose的便捷函数\n",
    "\n",
    "a = torch.tensor([[1,2,],[3,4]]) \n",
    "print(a)\n",
    "b = torch.transpose(a,0,1)\n",
    "print(b)\n",
    "c = torch.t(a)\n",
    "print(c)\n",
    "\n",
    "\n",
    "#torch.squeeze(input, dim, out=None)，移除默认所有size为1的维度，\n",
    "#当dim指定时，移除size为1的维度。返回的tensor会和input共享存储空间，所以任何一个的改变都会影响另一个\n",
    "#torch.unsequeeze(input, dim, out=None)，扩展input的size，如A*B变成1*A*B\n",
    "a = torch.tensor([[1,2,3,4]])\n",
    "d = torch.squeeze(a)\n",
    "print(d)\n",
    "e = torch.unsqueeze(d,dim=0)\n",
    "print(e)\n",
    "\n",
    "\n",
    "#torch.reshape(input, shape)，返回的size为shape具有相同数值的tensor，注意shape=(-1,)这种表述，-1表示任意的\n",
    "a = torch.tensor([1,2,3,4,5,6])\n",
    "b = a.reshape(2,-1)\n",
    "print(b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1., 2., 3.]), tensor([4., 5., 6.]))\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "source": [
    "#其他的一些操作\n",
    "\n",
    "#torch.where(condition,x,y)，根据condition的值来相应x,y的值，true返回x的值，false返回y的值\n",
    "#torch.unbind(tensor, dim=0)，返回tuple解除指定的dim的绑定，相当于按指定的dim拆分\n",
    "a = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "b = torch.unbind(a,dim=0)#将一个(2,3),分成两个（3）\n",
    "print(b)\n",
    "\n",
    "\n",
    "#toch.nonzero(input,out=None),返回非零值的索引，每一行都是一个非零值的索引值\n",
    "c = torch.nonzero(a)\n",
    "print(c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18]])\n",
      "tensor([[ 92, 114, 138],\n",
      "        [164, 192, 222]])\n",
      "tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n",
      "tensor([[0, 0, 0],\n",
      "        [1, 1, 1]])\n",
      "tensor([[ 1,  4,  9],\n",
      "        [16, 25, 36]])\n",
      "tensor([[ 1,  4,  9],\n",
      "        [16, 25, 36]])\n",
      "tensor([[  2.7183,   7.3891,  20.0855],\n",
      "        [ 54.5982, 148.4132, 403.4288]])\n",
      "tensor([[0.0000, 0.6931, 1.0986],\n",
      "        [1.3863, 1.6094, 1.7918]])\n",
      "tensor([[-1.0280,  1.5283, -0.5479,  0.8545],\n",
      "        [-0.0129,  0.5519,  0.8314, -1.8394],\n",
      "        [-1.4780, -0.1044, -0.2210, -0.3757]])\n",
      "tensor([[0.9720, 1.5283, 1.4521, 0.8545],\n",
      "        [1.9871, 0.5519, 0.8314, 0.1606],\n",
      "        [0.5220, 1.8956, 1.7790, 1.6243]])\n"
     ]
    }
   ],
   "source": [
    "#Tensor的操作\n",
    "#torch.tensor()根据后面给的data创建Tensor, Tensor类型根据数据进行推断\n",
    "#torch.Tensor()是默认的tensor类型(torch.FloatTensor)的简称\n",
    "\n",
    "\n",
    "\n",
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "b = torch.abs(a, out=None)#torch.abs()求绝对值\n",
    "print(b)\n",
    "#加法\n",
    "c = torch.tensor([[7,8,9],[10,11,12]])\n",
    "d = torch.tensor([[13,14,15],[16,17,18]])\n",
    "e = torch.add(a,c)#torch.add()实现两个tensor相加\n",
    "print(e)\n",
    "\n",
    "#torch.addcmul(tensor, value,tensor1,tensor2,out=None)\n",
    "#用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor上，\n",
    "#张量形状不需要匹配，但是元素数量必须一致\n",
    "f = torch.addcmul(a,1,c,d)\n",
    "print(f)\n",
    "\n",
    "#torch.addcdiv(tensor, value=1, tensor1, tensor2,out=None)\n",
    "#用tensor2对tensor1逐元素相除，然后乘以标量值value并加到tensor上\n",
    "g = torch.addcdiv(a,1,d,c)\n",
    "print(g)\n",
    "\n",
    "\n",
    "\n",
    "#除法\n",
    "#torch.div(input, value, out=None)，将Input逐元素除以标量值value，并返回结果到输出张量out\n",
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "c = torch.div(a,4)\n",
    "print(c)\n",
    "\n",
    "#乘法\n",
    "#torch.mul(input, other,out=None)，两个张量按元素相乘，并返回到输出张量，\n",
    "#两个张量的形状不须匹配，但是总元素数须一致。当形状不匹配时，input的形状作为输出张量的形状。\n",
    "b = torch.mul(a,a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "\n",
    "#幂运算\n",
    "#torch.pow(input,exponent, out=None), y_i = input^(exponent)\n",
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "b = torch.pow(a,2)\n",
    "print(b)\n",
    "\n",
    "\n",
    "#指数运算\n",
    "#torch.exp(tensor, out=None)，y_i = e^(x_i)\n",
    "#torch.expm1(tensor, out=None), y_i = e^(x_i) - 1\n",
    "#a = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "a = torch.Tensor([[1,2,3],[4,5,6]]) #创建的a默认的floatTensor类型\n",
    "#a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "#这个a是torch.tensor创建的，那么根据后面给的1,2,3等数据，将其判定为LongTensor，所以执行torch.exp()会报错\n",
    "b = torch.exp(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "#对数运算\n",
    "#torch.log(input, out=None), y_i = log_e(x_i)\n",
    "a = torch.Tensor([[1,2,3],[4,5,6]])#这里的a是floatTensor\n",
    "#a = torch.tensor([[1,2,3],[4,5,6]])会报错，因为这时a是longTensor\n",
    "b = torch.log(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "#截断函数\n",
    "#torch.ceil(input, out=None)，对输入input的每个元素向上取整\n",
    "#torch.floor(input, out=None)，对输入inout的每个元素向下取整\n",
    "#torch.round(input,out=None),返回相邻最近的整数，四舍五入\n",
    "#torch.trunc(input, out=None)，返回整数部分\n",
    "#torch.frac(input, out=None),返回小数部分\n",
    "#torch.fmod(input, divisor, out=None)，返回input/divisor的余数\n",
    "#torch.remainder(input, divisior, out=None)，返回input/divisior的除数\n",
    "a = torch.randn(3,4)\n",
    "print(a)\n",
    "b = torch.ceil(a)\n",
    "b = torch.floor(a)\n",
    "b = torch.round(a)\n",
    "b = torch.trunc(a)\n",
    "b = torch.frac(a)\n",
    "b = torch.fmod(a,2)\n",
    "b = torch.remainder(a,2)\n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max element index is: tensor(4)\n",
      "The min element index is: tensor(0)\n",
      "The dim=1 multipule result is: tensor([[  1.,   2.,   6.],\n",
      "        [  4.,  28., 168.]])\n",
      "The dim=0 multipule result is: tensor([[ 1.,  2.,  3.],\n",
      "        [ 4., 14., 18.]])\n",
      "The dim=0 add result is: tensor([[1., 2., 3.],\n",
      "        [5., 9., 9.]])\n",
      "The dim=1 add result is: tensor([[ 1.,  3.,  6.],\n",
      "        [ 4., 11., 17.]])\n",
      "The p distance between a and b is: tensor(4.1231)\n",
      "The element average is: tensor(3.6250)\n",
      "The element sum is: tensor(29.)\n",
      "The element median is: tensor(4.)\n",
      "The element mode is: (tensor(5.), tensor(7))\n",
      "The tensor std is: tensor(1.5059)\n",
      "The tensor var is: tensor(2.2679)\n",
      "The unique tensor is: tensor([4., 3., 5., 2., 1.])\n",
      "The multipule result is: tensor(12000.)\n"
     ]
    }
   ],
   "source": [
    "#tensor的降维操作\n",
    "a = torch.Tensor([[1,2,3],[4,7,6]])\n",
    "\n",
    "#torch.argmax(input, dim=None, keepdim=False),返回最大值的索引值\n",
    "b = torch.argmax(a)\n",
    "print(\"The max element index is:\", b)\n",
    "\n",
    "#torch.argmin(input, dim=None, keepdim=False)，返回最小值的索引值\n",
    "b = torch.argmin(a)\n",
    "print(\"The min element index is:\",b)\n",
    "\n",
    "#torch.cumprod(input, dim, out=None),\n",
    "#返回当前维度前面的所有元素连乘的结果，y_i = x_1*x_2*x_3...*x_i\n",
    "#这里dim的替换，一样思考a[i][j]，当dim=0，则变成a[i+1][j]，当dim=1，则变成a[i][j+1]\n",
    "b = torch.cumprod(a,1)\n",
    "c = torch.cumprod(a,0)\n",
    "print(\"The dim=1 multipule result is:\",b)\n",
    "print(\"The dim=0 multipule result is:\",c)\n",
    "\n",
    "#torch.cumsum(input, dim, out=None),\n",
    "#返回当前维度前面的所有元素相加的结果，y_i = x_1 + x_2 +...+x_j\n",
    "#这里的dim思考方式同上\n",
    "b = torch.cumsum(a,dim=0)\n",
    "c = torch.cumsum(a, dim=1)\n",
    "print(\"The dim=0 add result is:\", b)\n",
    "print(\"The dim=1 add result is:\", c)\n",
    "\n",
    "#torch.dist(input, out, p=2)\n",
    "#返回input和output的p式距离,差值的平方和再开方\n",
    "a = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "b = torch.Tensor([[1,3,5],[6,7,8]])\n",
    "c = torch.dist(a,b)\n",
    "print(\"The p distance between a and b is:\",c)\n",
    "\n",
    "\n",
    "\n",
    "#torch.mean()，返回平均值\n",
    "#torch.sum()，返回总和值\n",
    "#torch.median()，返回中间值\n",
    "#torch.mode()，返回众数值\n",
    "#torch.std()，返回标准差\n",
    "#torch.var()，返回方差\n",
    "#torch.unique(input, sorted=False)，去重操作，相当于取set，返回一个1-D的唯一的tensor\n",
    "#torch.prod(input, dim, keepdim=False)，返回指定维度的乘积结果，一个值\n",
    "\n",
    "a = torch.Tensor([1,2,3,4,4,5,5,5,])\n",
    "b = torch.mean(a)\n",
    "c = torch.sum(a)\n",
    "d = torch.median(a)\n",
    "e = torch.mode(a)\n",
    "f = torch.std(a)\n",
    "g = torch.var(a)\n",
    "h = torch.unique(a)\n",
    "i = torch.prod(a)\n",
    "print(\"The element average is:\",b)\n",
    "print(\"The element sum is:\",c)\n",
    "print(\"The element median is:\",d)\n",
    "print(\"The element mode is:\",e)\n",
    "print(\"The tensor std is:\",f)\n",
    "print(\"The tensor var is:\",g)\n",
    "print(\"The unique tensor is:\",h)\n",
    "print(\"The multipule result is:\",i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of torch.equal: True\n",
      "The result of torch.equal: False\n"
     ]
    }
   ],
   "source": [
    "#tensor的对比操作\n",
    "\n",
    "#torch.equal(tensor1, tensor2)，如果tensor1与tensor2有相同的size和elements，则为true，否则为false\n",
    "a = torch.equal(torch.tensor([1,2,3]), torch.tensor([1,2,3]))\n",
    "b = torch.equal(torch.tensor([1,2,3]), torch.tensor([1,2,2]))\n",
    "print(\"The result of torch.equal:\",a)\n",
    "print(\"The result of torch.equal:\",b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of torch.eq is: tensor([[1, 0],\n",
      "        [1, 0]], dtype=torch.uint8)\n",
      "The result of torch.ge is: tensor([[1, 0],\n",
      "        [1, 0]], dtype=torch.uint8)\n",
      "The result of torch.gt is: tensor([[0, 0],\n",
      "        [0, 0]], dtype=torch.uint8)\n",
      "The result of torch.ne is: tensor([[0, 1],\n",
      "        [0, 1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "#torch.eq(tensor1, tensor2,out=None),按照每个元素进行对比，如果相等则为1，如果不等则为0\n",
    "a = torch.eq(torch.tensor([[1,2],[3,4]]), torch.tensor([[1,3],[3,5]]))\n",
    "print(\"The result of torch.eq is:\",a)\n",
    "\n",
    "#torch.ge(input, other,out=None),input >= other则为1，否则为0\n",
    "a = torch.ge(torch.tensor([[1,2],[3,4]]), torch.tensor([[1,3],[3,5]]))\n",
    "print(\"The result of torch.ge is:\",a)\n",
    "\n",
    "#torch.gt(input, other, out=None),input>other则为1，否则为0\n",
    "a = torch.gt(torch.tensor([[1,2],[3,4]]), torch.tensor([[1,3],[3,5]]))\n",
    "print(\"The result of torch.gt is:\", a)\n",
    "\n",
    "#同理torch.le(input, other, out=None)，input<=other则为true，否则为false\n",
    "#torch.lt(input, other, out=None)，input<other则为true，否则为false\n",
    "\n",
    "#torch.ne(input, other, out=None)，只有当input != other时才为1，相等时为0\n",
    "a = torch.ne(torch.tensor([[1,2],[3,4]]), torch.tensor([[1,3],[3,5]]))\n",
    "print(\"The result of torch.ne is:\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of torch.cross is: tensor([[-3,  0,  1],\n",
      "        [ 0,  0,  0]])\n",
      "The result of torch.dot is: tensor(36)\n",
      "The result of torch.mm is: tensor([[ 7, 13],\n",
      "        [15, 29]])\n",
      "The result of torch.eig is: (tensor([[-0.3723,  0.0000],\n",
      "        [ 5.3723,  0.0000]]), tensor([]))\n",
      "The result of torch.det is: tensor(-2.0000)\n",
      "The result of torch.trace is: tensor(5.)\n",
      "The result of torch.diag is: tensor([1., 4.])\n",
      "The result of torch.diag is: tensor([[1., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 3., 0.],\n",
      "        [0., 0., 0., 4.]])\n",
      "The result of torch.histc is: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "The result of torch.tril is: tensor([[1, 0, 0],\n",
      "        [4, 5, 0],\n",
      "        [7, 8, 9]])\n",
      "The result of torch.triu is: tensor([[1, 2, 3],\n",
      "        [0, 5, 6],\n",
      "        [0, 0, 9]])\n"
     ]
    }
   ],
   "source": [
    "#tensor的其他一些操作\n",
    "\n",
    "#torch.cross(input, other, dim=-1, out=None)，实现叉乘，外积\n",
    "#这里是返回沿着维度dim上，两个张量Input的叉积，Input和other必须具有相同的形状，且指定的dim维度上size必须为3，\n",
    "#如果不指定dim，则默认为第一个尺度为3的维度\n",
    "#这个的具体计算暂时还未弄清楚，向量的外积和内积的区别\n",
    "a = torch.cross(torch.tensor([[1,2,3],[3,4,5]]), torch.tensor([[1,3,3],[3,4,5]]))\n",
    "print(\"The result of torch.cross is:\", a)\n",
    "\n",
    "#torch.dot(tensor1, tensor2)，\n",
    "#返回tensor1和tensor2的点乘,其中tensor1和tensor2都必须为1D向量，对位相乘在相加\n",
    "a = torch.dot(torch.tensor([1,2,3,4]), torch.tensor([1,3,3,5]))\n",
    "print(\"The result of torch.dot is:\", a)\n",
    "\n",
    "\n",
    "#torch.mm(mat1, mat2, out=None),返回矩阵mat1和mat2的乘积,注意他执行的是矩阵的乘法\n",
    "a = torch.mm(torch.tensor([[1,2],[3,4]]), torch.tensor([[1,3],[3,5]]))\n",
    "print(\"The result of torch.mm is:\", a)\n",
    "\n",
    "\n",
    "#torch.eig(a, eigenvectors=False, out=None)，返回矩阵a的特征值和特征向量\n",
    "a = torch.eig(torch.Tensor([[1,2],[3,4]]))\n",
    "print(\"The result of torch.eig is:\", a)\n",
    "\n",
    "\n",
    "#torch.det(A)，返回矩阵A的行列式\n",
    "a = torch.det(torch.Tensor([[1,2],[3,4]]))\n",
    "print(\"The result of torch.det is:\",a)\n",
    "\n",
    "#torch.trace(input)，返回2D矩阵的迹\n",
    "a = torch.trace(torch.Tensor([[1,2],[3,4]]))\n",
    "print(\"The result of torch.trace is:\",a)\n",
    "\n",
    "#torch.diag(input, diagonal=0, out=None),输出也是tensor\n",
    "#如果输入是一个向量，则返回一个以input为对角线元素的2D方阵,其他元素为0\n",
    "#如果输入是一个矩阵，则返回一个包含input为对角元素的1D张量\n",
    "#如果diagonal=0，返回的是主对角线上的元素\n",
    "#如果diagonal>0，则返回的是主对角线之上的元素\n",
    "#如果diagonal<0，则返回的是主对角线之下的元素\n",
    "\n",
    "a = torch.diag(torch.Tensor([[1,2],[3,4]]), diagonal=0)\n",
    "b = torch.diag(torch.Tensor([1,2,3,4]), diagonal=0)\n",
    "print(\"The result of torch.diag is:\", a)\n",
    "print(\"The result of torch.diag is:\",b)\n",
    "\n",
    "\n",
    "#torch.histc(input, bins=100, min=0, max=0, out=None)\n",
    "#计算input的直方图，如果min和max都为0，则利用数据中的最大最小值作为边界\n",
    "#这个没弄明白具体的计算细节\n",
    "a = torch.histc(torch.Tensor([1,2,3,4]))\n",
    "print(\"The result of torch.histc is:\",a)\n",
    "\n",
    "\n",
    "\n",
    "#torch.tril(input, diagonal=0, out=None),返回矩阵的下三角矩阵，其他为0\n",
    "#torch.triu(input, diagonal=0, out=None），返回矩阵的上三角矩阵，其他为0\n",
    "\n",
    "a = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b = torch.tril(a)\n",
    "c = torch.triu(a)\n",
    "print(\"The result of torch.tril is:\",b)\n",
    "print(\"The result of torch.triu is:\",c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
