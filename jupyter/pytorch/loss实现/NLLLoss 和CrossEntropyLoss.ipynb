{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLLoss的全称是Negative Log Likelihood Loss，也就是最大似然函数。\n",
    "\n",
    "在图片进行单标签分类时，【注意NLLLoss和CrossEntropyLoss都是用于单标签分类，而BCELoss和BECWithLogitsLoss都是使用与多标签分类。这里的多标签是指一个样本对应多个label.】\n",
    "\n",
    "假设输入m张图片，输出一个m*N的tensor,其中N是分类的个数，比如N为词表大小。比如，输入3张图片，分三类，最后的输出是一个$3*3$的tensor，举一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input = torch.randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2646, -0.3598, -0.3506],\n",
       "        [-1.4290, -0.0606,  1.6310],\n",
       "        [-0.5436, -0.3721,  0.6544]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设每一行对应一个样本在3个类别上的输出值，接下来我们可以使用Softmax，来得到每张图片的概率分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_result = softmax(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4817, 0.2580, 0.2604],\n",
       "        [0.0381, 0.1496, 0.8123],\n",
       "        [0.1818, 0.2158, 0.6024]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后再对softmax的结果取对数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_result = torch.log(softmax_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7305, -1.3549, -1.3457],\n",
       "        [-3.2680, -1.8995, -0.2079],\n",
       "        [-1.7049, -1.5333, -0.5069]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLLoss的结果就是把上面的输出log_result与Label对应的值拿出来，去掉负号再求和取平均。\n",
    "\n",
    "假设target=[1,0,2]，所以就是取出-1.3549, -3.2680, -0.5069，去掉负号再求和取平均。\n",
    "\n",
    "具体结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.709933333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.3549 + 3.2680 + 0.5069)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面用NLLLoss来验证一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7099)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fun = nn.NLLLoss()\n",
    "target = torch.tensor([1,0,2])\n",
    "loss = loss_fun(log_result, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到结果是基本一致的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropyLoss就是交叉熵代价函数。\n",
    "\n",
    "它就是把上面的我们执行的softmax+log+NLLLoss合并起来了，一步执行完。\n",
    "\n",
    "我们可以来验证一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun2 = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2 = torch.tensor([[ 0.2646, -0.3598, -0.3506],\n",
    "        [-1.4290, -0.0606,  1.6310],\n",
    "        [-0.5436, -0.3721,  0.6544]])\n",
    "target = torch.tensor([1,0,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7099)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cross = loss_fun2(input2, target)\n",
    "loss_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到结果是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结： NLLLoss和CrossEntropyLoss计算损失函数的形式可以统一为：\n",
    "$$loss(y_{model},y_{true}) = -\\sum y_{true}*log(y_{model})$$\n",
    "\n",
    "其中$y_{model}$表示模型的输出（经过softmax后的结果）， $y_{true}$表示样本的真实target.\n",
    "\n",
    "在具体的训练中，其实target是个N维tensor，只有对应的Label位为1，其他的都为0，$y_{model}$也是一个N维的tensor,\n",
    "这样就实现了上面说的只选取Label位的值参与最后的求和取平均计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
