{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BPE的思想是借鉴subword\n",
    "subword是word和char之间的一个中间层，考虑的是从形态学的角度来对词的含义进行表征。这里主要是考虑到了几个word embedding方法的不足：\n",
    "    --1、在训练词向量时，如果某个word出现的次数比较少的话，那么它的更新次数也会比较少，这样就很难学到这个word的高质量的向量表示；\n",
    "    --2、有些词过于稀有，没有在预训练词向量的语料中出现，这样就会导致预测结果无法得到这个词。以人名为例，假设在做阅读理解\"His name is Mikolov,...,__ is a NLP expert.\"这一整段话都是对Mikolov的描述，最后留个空格显然也要填入Mikolov，然而由于这个姓氏太过于稀有，没有在训练语料中出现，所以模型也就无法预测得到正确答案；\n",
    "    --3、单词的构成可能会包含一些前缀和后缀的信息(eg: pre-、sub-、-er、-est)。单词的构成也可能是由不同的成分组合而成的(eg:bio+informatics)。这些信息在一定程度上也能够表示单词的含义，然而在word-level的embedding中，这些信息往往会被忽略掉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而subword embedding的提出则能很好的解决上述的问题，在此介绍一下两种比较常见的subword-level embedding的方法：\n",
    "\n",
    "N-gram\n",
    "    顾名思义，就是以一个固定长度的滑动窗口去对单词的子词进行截取,比如：apple -> (ap、app、ppl、ple、le)，最后再将各个subword的向量求和就可以得到整个word的向量表示。\n",
    "    \n",
    "BPE\n",
    "    BPE算法是94Nian1Gage等人提出的，但是\"Neural machine translation of rare words with subword unit\"这篇nmt文章将其用到了subword-level上。我们可以知道n-gram方法虽然能够解决上诉的word-level embedding的问题，但是它由于是滑动窗口采样的，会导致存在大量冗余信息，也会导致词表大小增大导致算法运行效率变慢。\n",
    "    因此，如果我们对常用词采用word-level向量的表示，稀有词再用subword-level向量的表示，就可以很好的解决上诉问题，因此作者提出用subword-level的BPE算法来解决这个问题。\n",
    "    BPE算法的思想其实就是，首先将各个单字符初始化为token，在统计一下两两相邻的token的出现次数，将次数最大的token pair给合并起来成为新的token，放回继续统计和合并，最后得到非重叠的subword。\n",
    "    经过这种组合方式，常见词最终会由char回归到word级别，而稀有词则会在subword层面上就停止了合并，也就达到了我们的目的。比如unoffical拆成un+offical的组合，进而得到高质量的词向量表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pairs is: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 6, ('t', '</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3, ('s', 't</w>'): 3})\n",
      "The best is: ('e', 's')\n",
      "word is: l o w </w>\n",
      "w_out is: l o w </w>\n",
      "word is: l o w e r </w>\n",
      "w_out is: l o w e r </w>\n",
      "word is: n e w e s t </w>\n",
      "w_out is: n e w es t </w>\n",
      "word is: w i d e s t</w>\n",
      "w_out is: w i d es t</w>\n",
      "######################\n",
      "The pairs is: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 6, ('t', '</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3, ('es', 't</w>'): 3})\n",
      "The best is: ('l', 'o')\n",
      "word is: l o w </w>\n",
      "w_out is: lo w </w>\n",
      "word is: l o w e r </w>\n",
      "w_out is: lo w e r </w>\n",
      "word is: n e w es t </w>\n",
      "w_out is: n e w es t </w>\n",
      "word is: w i d es t</w>\n",
      "w_out is: w i d es t</w>\n",
      "######################\n",
      "The pairs is: defaultdict(<class 'int'>, {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 6, ('t', '</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3, ('es', 't</w>'): 3})\n",
      "The best is: ('lo', 'w')\n",
      "word is: lo w </w>\n",
      "w_out is: low </w>\n",
      "word is: lo w e r </w>\n",
      "w_out is: low e r </w>\n",
      "word is: n e w es t </w>\n",
      "w_out is: n e w es t </w>\n",
      "word is: w i d es t</w>\n",
      "w_out is: w i d es t</w>\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    #统计两个相邻字符及其出现次数等\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        print(\"word is:\", word)\n",
    "        print(\"w_out is:\", w_out)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "\n",
    "vocab = {'l o w </w>': 5, 'l o w e r </w>':2, 'n e w e s t </w>':6, 'w i d e s t</w>':3}\n",
    "num_merges = 3\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    print(\"The pairs is:\", pairs)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    #获取value最大的key\n",
    "    print(\"The best is:\",best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(\"######################\")\n",
    "\n",
    "\n",
    " #这段代码的核心思路就是先将word按照空格来区分开，然后将出现最多的合并在一起当成一个字符，然后不断循环迭代；   \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www\\.python\\.org\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#re.escape(pattern)可以对字符串中所有可能被解释为正则运算符的字符进行转义的应用函数。\n",
    "s = \"www.python.org\"\n",
    "res = re.escape(s)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='Hello'>\n",
      "<class 're.Match'>\n",
      "Hello\n",
      "(0, 5)\n"
     ]
    }
   ],
   "source": [
    "#re.compile()的运用\n",
    "import re\n",
    "\n",
    "def main():\n",
    "    content = \"Hello, I am Jerry, from Chongqing, a montain city, nice to meet you...\"\n",
    "    regex = re.compile('\\w*o\\w*')\n",
    "    y = regex.match(content)\n",
    "    print(y)\n",
    "    print(type(y))\n",
    "    print(y.group())\n",
    "    print(y.span())\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
