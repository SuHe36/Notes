A Hybrid Retrieval-genration neural conversation model

这篇论文中可以先找出检索与生成的各自的优点

然后可以使用图1作为介绍将生成与检索融合的一个例子

Context-Encoder
作者首先将一个有多条句子的上下文contexn输入到context encoder中，这个encoder是个2层的的LSTM.
作者是把这多条上下文一起合并输入的，对每一个时刻都保留了它的最后LSTM的隐层结果作为输出。

Facts Encoder
这里作者是对于有多条facts，分别输入到一个2层的LSTM的encoder中，然后对于每条facts，使用对每个时刻的输出求均值来获得最后的表示

Response decoder
这个decoder主要是基于context_encoder的结果、facts_encoder的结果，还有前面时刻1~t-1的生成结果，来生成t时刻的token
作者把context_encoder和facts_encoder的结果concate在一起了，最后的维度就是H*(Lu+F)，其中H是隐层维度。如768等,Lu是context_words的长度，F是facts的语句数目。然后计算矩阵E和st-1之间的attention分布，再对E矩阵进行对位相乘再相加，得到糅合st-1信息的新表示。最后就把前一时刻的表示st-1和ct合并在一起，再用一个tanh层

然后t0使用context_word的最后一个词的表示，以及Fcats的均值来生成的，最外面的fai函数是一个线性变换，一个dense层

然后用一个ground truth response y, 去训练这两个encoder和一个decoder


Retrieval module:
作者首先将(context, response)对，作为训练语料，然后用Lucene使用context去检索相应的response. 选取topk作为最后的结果


Hybrid Rerank
作者使用generate的一个句子和topk个检索出来的结果作为一个候选集，然后对于每一个候选项，作者先计算他们的word_embedding表示与context中的每个词的相似度（dot product），然后再把这个相似度矩阵输入到一个CNN层，然后在解 再输入到一个MLP层，得到一个match score

这里其实对于这k+1个候选并没有真实的label，所以作者采取了一种distant supervision的方式，作者使用BLEU或者rouge-L的评估标准，计算每一个候选和ground truth的分值，分值高的k'个作为正例，其余的作为负例。其中k'的取值为1,2,3等


Response Generation by Context-aware Prototype Editing
这个可以贴上它的Table 1，作为例子进行介绍

作者首先是对于一个给与的context c，然后检索与它相近的context c'以及他相对应的r'，
然后利用c和c'计算出一个edit vector，用这个edit vector去指导r'生成真正有效的response

这句话可以写上：
Our idea is that differences between responses strongly cor- relates with differences in their contexts

可以再写一下p(x|x')变成了p(x|x',z)

在介绍insert word set 和delete word set

Prototype Selector
作者按照c和c'的相似度去检索(c', r')，作者采用的是Lucene去建立index, 

在训练阶段，因为已经知道了ground truth，所以作者采取的是使用r去检索与它相似度相近的r', 选择Jacard similarity在[0.3, 0.7]之间的。
选取top20的作为候选，丢弃<0.3的是因为候选和真实之间差别过大，而丢弃>0.7，是因为为了防止这个edit操作可能是个直接copy原句的操作。还有一个原因是因为，即使是相近的context，他们的回复也很有可能不同，所以为了让我们的模型是在只是词不同的句子上进行操作，作者在训练阶段选择r来检索(c', r')。作者也进行了实验，如果训练阶段按照c来检索的话，那么edit操作总是生成一些没有意义的回复。



edit vector generation：
    首先构建一个diff_c向量，先把insett词表和delete词表取得他们的word_emdedding, 然后按照attention分布相乘，
    那里面的hl是r'的最后一个隐层时刻的表示，所以作者其实是没有在专门对c和c'进行编码
    然后再把他们concate在一起，最后再和一个矩阵w相乘映射回去得到edit vector: z

Prototype Editing
    decoder的输入是检索出来的r'

    作者显示使用r_{j-1}和z生成一个新的source R’的隐层表示h'_j，公式9
    然后图里的hj和公式10~12里的hj，我就理解为是前面的公式2生成的R'的隐层表示
    然后公式10~12可以理解为，有了当前糅合z向量的h'i，然后对于当前时刻的



    作者先将edit vector 和 r_j-1【这个r_j-1是检索出来的r'里的单词】的word embedding concate在一起，把他们作为GRU的输入，得到r的一个新的GPU表示
    然后还有前面的只对r'编码获得的一个隐层表示
    这里作者计算了h'i对0~t时刻的r的一个attetion分布，然后在得到一个糅合了attention分布的h1~ht的表示ci。
    最后再把ci和hi'以及ri-1合并在一起，进行softmax预测词表分布。
    所以最后预测softmax词表分布时，主要来源有三个，一个是ri-1的embedding表示，一个是糅合了edit vector的h'i表示，
    一个是糅合了h0~ht的attention表示

实验：
    在这篇文章中，作者只考虑了single turn response
    作者采取的是从douban中搜集的数据，其中19623374 pairs作为训练集
    然后测试和验证集中，选取c'和r'是按照c'与c的相似度来进行选择的。

Retrieval-Guided Dialogue Response Generation via a Matching-to-Generation Framework, 2019


