先生成model_
data_path：../training_data/dif_models_fr



concat_finetuning：将ori和rule结合起来一起进行训练
    --生成model_path：models_cat_fr/ori_rule/
    --这个init_model_path，我认为是导入预训练的模型参数，这里是./models/formality_infer/，应该可以换成./models/117M
    --data_path：../training_data/dif_models_fr/，里面存储的是fr领域的ori和rule数据
    --cat_files里的：有[informal.train.ori，informal.train.rule, formal.train.rule]，train.ori_rule

有空格和tab问题时，可以使用下面的命令
python -m tabnanny yourfile.py



tf导入模型训练的整个流程：NMT
    - train函数
      - 定义gpt2    = NMT_GPT(input_num=2, config_path='./models/117M/'),进行初始化，定义模型有哪些参数
      - 定义trainer = NMT_GPT_Trainer(gpt2)，定义整个训练的graph结构
        - build_graph（）:建立图结构
          - 这里调用了NMT_GPT.build_training_model（）来建立整个graph结构
        -create_session_init_and_print_all_trainable_vars():
          -这一步相当于把self.vars_for_infer里面的变量的值全部换成'./models/117M'里的模型的值
        - self.vars_for_train只有两类变量'beta'和'adam'
      - 调用trainer.build_graph()来构建整个模型结构图
      - trainer.training()整个训练开始
        - 调用create_session_init_and_print_all_trainable_vars(),将self.vars_for_infer里面的变量的值全部换成'./models/117M'里的模型的值
        - 然后调用restore_model_and_init()
          - 这一步直接使用了self.saver_infer.restore()，用'./models/117M'对self.saver_infer进行了初始化
          - 同时还对self.saver_train()进行了初始化
          - **这一步其实不是很明白**
        - 后面就是训练过程，以及每多少步在验证集上评估效果


test的整个过程：
    -test函数
        - 定义gpt2 = NMT_GPT()进行初始话，定义模型有哪些参数
        - beam_search_generator()，也是进行初始化，定义模型的结构和导入模型
          - 这个位于single_gpu_serving下面
        - generator.build_graph_and_restore()，定义模型结构和导入参数
          - 调用NMT_GPT()的build_beam_search_graph()来定义图结构，这里与train的结构不同只在于decode返回一个词
          - 这里用tf.train.latest_checkpoint和saver,restore()重新导入模型


