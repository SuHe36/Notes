## 概述
监督学习方法可以分成生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。

常见的判别模型有：线性回归，线性判别分析，支持向量机，boosting，条件随机场，神经网络等；

常见的生成模型有隐马尔科夫模型，朴素贝叶斯模型，高斯混合模型，LDA等。

## 判别模型
判别方法由数据直接学习决策函数：
$$f(x)$$

或者条件概率分布：
$$P(y|x)$$

作为预测的模型，即判别模型。判别方法关心的是对给定输入$x$，应该预测出什么样的输出$y$。

比如说要确定一只羊是山羊还是绵羊，用判别模型的方法是先从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，还是绵羊的概率。


## 生成模型
**生成方法由数据学习输入和输出联合概率分布：**
$$P(x,y)$$

然后求出后验概率分布:
$$P(y|x)$$

作为预测的模型，即生成模型。

这里以朴素贝叶斯为例，我们要求的目标可以通过：
$$P(x,y)=P(x|y)P(y)$$

求出输入输出的联合概率分布，然后通过贝叶斯公式：
$$P(y|x) = \frac{P(x|y)P(y)}{P(x)}$$

**求出后验概率分布。**

对于上面的例子我们换种思路，我们可以根据山羊的特征首先学习出一个山羊模型，然后根据绵羊的特征学习出一个绵羊模型。然后根据从这只羊中提取出的特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是哪个。

由于我们关注的是y的离散值结果中哪个概率大（比如山羊概率和绵羊概率哪个大），而并不是关心具体的概率，因此上式改写成：

$$arg \max_{y}P(y|x) = arg \max_{y} \frac{P(x|y)P(y)}{P(x)} = arg \max_{y}P(x|y)P(y)$$

其中$P(x|y)$称为似然函数，P(y)称为先验概率，P(x)是用于归一化的证据因子，对于给定的样本x，证据因子P(x)与类标记无关，因此估计P(y|x)的问题就转化为如何基于训练数据来估计先验P(y)和似然P(x|y)。


## 判别模型与生成模型比较
判别模型：

优点：
- 仅需要有限的样本。节省计算资源，需要的样本数量也少于生成模型。
- 能清晰的分辨出多类或某一类与其他类之间的差异特征，准确率往往较生成模型高。
- 由于直接学习P(y|x)，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维，构造等），从而能够简化学习问题。

缺点：
- 不能反映训练数据本身的特性。可以告诉你是1还是2，但是没法把整个场景描述出来【比如后面生成模型可以求出边缘分布P(x)来进行异常值检测，而分类模型就无法求出来】。
- 没有生成模型的有点。
- 黑盒操作：变量间的关系不清楚，不可视。


生成模型：

优点：

- (1)，生成给出的是联合分布:
$$P(x,y)$$
不仅能够由联合分布计算后验分布：
$$P(y|x)$$

还可以给出其他信息，比如可以使用:
$$P(x)=\sum_{i=1}^k P(x|y_i)P(y_i)$$

来计算边缘分布:
$$P(x)$$

如果一个输入样本的边缘分布P(x)很小的话，那么可以认为学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好，这也是所谓的异常值检测（outlier detection， 如果一个x的P(x)特别低，就可以认为它是异常值）。

- (2)，生成模型收敛速度比较快，即当样本数量较多时，生成模型能够更快的收敛于真实模型。
- (3)，生成模型能够应对存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。
- (4)，研究单类问题比判别模型灵活性更强。


缺点：
- (1)，联合分布能够提供更多的信息，但也需要更多的样本和更多的计算，尤其是为了更准确的估计类别条件分布，需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到的，因而如果我们只需要做分类任务，就浪费了计算资源。
- (2)，另外，实践中多数情况下判别模型的效果会更好。





