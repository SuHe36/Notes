随机森林算法比较简单，这里对它做一个小结。

随机森林是集成学习的一个子类，它依靠于决策树的投票选择来决定最后的分类的结果。

随机森林中会有很多的分类树。我们要对一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类，对每棵树的分类结果进行投票，获取票数最多的类别就是森林的分类结果【这里也就是说每棵树的投票权重是一样的】。并且森林中的每棵树都是相互独立的。

将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林的bagging的思想。

有了树我们就可以进行分类，但是森林中的每棵树是怎么生成的呢？

每棵树按照如下规则进行生成：

1） 如果训练集的大小为N，那么对于每棵树而言，就需要随机的从训练集中抽取N个训练样本【假设训练集的总规模为n，大于N，这种采样方式叫做bootstrap sample】，作为该树的训练集。
   
从这里我们可以知道：每棵树的训练集是不同的，而且里面包含重复的训练样本。

**为什么要随机的抽取训练集？**

如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出来的树分类结果也是完全一样的，这样的话就没有bagging的必要。

**为什么要有放回的抽样？**

我的理解是这样的，如果不是有放回的抽样。那么每棵树的训练样本就都是不相同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后的分类结果取决于多棵树（弱分类器）的投票结果，**这种表决应该是求同，因此使用完全不同的训练集来训练每棵树这样对最终分类是没有帮助的。**

2）如果每个样本的特征维度为M，那么指定一个常数m<< M，随机的从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的。

3） 每棵树都尽最大程度的生长，并且没有剪枝的过程。

一开始我们提到的随机森林的"随机"，就是指这两个随机过程：
- 有放回的随机抽取训练样本
- 随机的从M个特征维度中选取m个特征维度参与树的分裂

随机森林的分类效果（错误率）与两个因素有关：
- 森林中的任意两棵树的相关性：相关性越大，错误率越大
- 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率也就越低

减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。**所以关键问题是如何选择最优的m，这也是随机森林的唯一参数。**

## 袋外错误率

正如上面提到的，构建随机森林的关键是如何选择最优的m，要解决这个问题主要依据是计算袋外错误率oob error（out-of-bag error）。

随机森林有一个很重要的有点就是，没有必要对他进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。
它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。

我们知道，在构建每棵树的时候，我们对训练集使用了不同的bootstrap sample(随机且有放回的抽样)，所以对于每棵树而言（假设是对于第k颗树），大约有1/3的训练实例没有参与到第k棵树的生成，它们就是第k棵树的oob样本。

而这样的采样特点就允许我们进行oob估计，它的计算方式如下：
1. 对每个样本，计算他作为oob样本的树的分类情况（大约1/3的树）
2. 然后以简单多数投票作为该样本的分类结果
3. 最后用误分个数占样本总数的比率作为随机森林的oob误分率


