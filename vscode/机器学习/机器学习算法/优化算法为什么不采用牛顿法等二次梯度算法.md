牛顿法与传统的SGD等一阶梯度算法相比，需要计算二阶导数，也就是要计算一个Hessian矩阵。
- 在样本特征维度较大时，比如数据集有n个特征，那么Hessian矩阵就是n\*n的，我们需要存储n\*n个值，而SGD等只需要存储n个点。这在空间上是一个负担
- 其实，牛顿法的迭代过程中需要对Hessian矩阵求逆，这也是非常耗时和耗费空间的一件事。
- 然后虽然牛顿法的优势是二阶收敛速度【考虑到梯度下降最快的方向】，但是在局部最优值附近，因为他是用一个二次曲面去拟合一次曲面，同时牛顿法的步长是通过导数计算得来的，所以在邻近鞍点的时候，步长会变的越来越小。这样牛顿法容易陷入局部最小值上，跳不出去。