# SVM总结

## 线性支持向量机
对于二维平面内的一个线性可分的数据，我们可以使用PLA/pocket算法在平面上把正负类分开，显然这样的直线不止一条，那么哪条直线更好呢。一般情况下，最好让分类直线和正类负类的点都有一定的距离，距离越远，可以表明分类直线对数据误差的容忍度越高。

**如何定义分类直线对数据误差的容忍度呢，就是看距离分类直线最近的点与分类直线的距离，我们把它用margin表示。**
分类直线由权重w决定，目的就是找到使margin最大时对应的w值。整体来说，我们的目标就是找到这样的分类线并满足下列条件：
- 分类正确，即$y_nw^Tx_n > 0$
- margin最大化

### 标准的Large Margin 问题
要让margin最大，即让离分类线最近的点到分类线距离最大，我们先来看一下如何计算点到分类线的距离。
首先，我们将权重$w(w_0, w_1,...,w_d)$中的$w_0$拿出来，用b表示。同时省去$x_0$项。这样，hypothesis就变成了$h(x)=sign(w^Tx+b)$
下面，我们用图解的方式，来推导点到分类平面的距离（其实也可以直接代用高中学的点到面的距离公式）:
![](../figure/21.png)
如上所示，假设平面wx+b=0上有两个点$x'和x''$，所以它们满足$w^Tx'+b=0$和$w^Tx''+b=0$，代入转化得到：
$$w^T(x''-x')=0 $$
其中$(x''-x')$是平面上的任一向量，$(x'-x'')$与w的内积为0，那么w就是平面的法向量，那么我们现在就是要计算平面外的一点x到该平面的距离，做法是只要将向量(x-x')投影到垂直于该平面的方向上(也就是法向量w的方向)上就可以了，那么距离就可以表示为：
$$ distance(x,b,w) = |(x-x')\cos(\theta)| = | ||x-x'||\cdot \frac{(x-x')w}{||x-x'||\cdot ||w||}| = \frac{1}{||w||}|w^Tx - w^Tx'| $$

然后代入$w^Tx'=-b$可得：
$$distance(x,b,w) = \frac{1}{||w||}|w^Tx+b|$$
点到分类面的距离也就算出来了。基于这个分类面，所有的点均满足:$y_n(w^Tx_n +b) > 0$，表示所有的点都分类正确，则distance公式就可以变换为：
$$distance(x,b,w) = \frac{1}{||w||}y_n(W^Tx_n +b) $$

那么，我们的目标形式就可以转换为：
- $$\mathop{\max} \limits_{b,w} \qquad margin(b,w)$$
- $$subject to \quad every \quad y_n(w^Tx_n+b)>0;$$

  $$ \qquad \qquad \quad margin(b,w) = \mathop{\min }\limits_{n=1,...,N}\frac{1}{||w||}y_n(w^Tx_n +b)$$
  
对于上面的式子还不容易求解，我们可以继续对它进行简化，我们知道分类面$w^Tx+b=0和3w^Tx+3b=0$其实是一样的。也就是说，对w和b进行同样的缩放还会得到同一分类面。
所以，为了简化计算，我们令距离分类面最近的点满足$y_n(w^Tx_n+b)=1$，那我们所要求的margin就变成了：
$$ margin(b,w) = \frac{1}{||w||} $$

这样的话，我们的目标形式就可以简化为：
- $$\mathop{\max} \limits_{b,w} \qquad \frac{1}{||w||}$$
- $$subject to \quad every \quad y_n(w^Tx_n+b)>0;$$
  $$ \qquad \qquad \quad \mathop{\min }\limits_{n=1,...,N} y_n(w^Tx_n +b)=1$$

因为距离分类面最近的点满足$y_n(w^Tx_N+b)=1$，也就是说对所有的点满足$y_n(w^Tx_n+b) \geq 1$，同时，因为我们最熟悉的还是最小化问题，所以我们可以把最大化目标$\frac{1}{||w||}$转化为最小化$\frac{1}{2}w^Tw$。
- $$ \mathop{\min}\limits_{b,w} \quad \frac{1}{2}w^Tw $$
- $$ subject to \quad y_n(w^Tx_n +b) \geq 1 for all n $$
也就是说，最终的条件就变成了$y_n(w^Tx_n +b) \geq 1$，我们的目标也就是最小化$\frac{1}{2}w^Tw$的值。


### 支持向量机(Support Vector Machine)
为什么把分类面仅由分类面的两边距离它最近的几个点决定的方法叫做支持向量机呢，这是因为其他的点对分类面没有影响，**决定分类面的几个点也称之为支持向量**，就好比这些点支撑着分类面。而利用Support Vector得到最佳分类面的方法，就称之为支持向量机。
下面介绍支持向量的一般求解方法，先写下我们的条件和目标：
-  $$\mathop{\min}\limits_{b,w} \quad \frac{1}{2}w^Tw $$
- $$subject to \quad y_n(w^Tx_n+b) \geq 1 for all n $$
这是一个典型的二次规划问题，因为SVM的目标是关于w的二次函数，条件是关于w和b的一次函数，它的求解过程还是比较容易的，可以使用一些软件，如matlab自带的二次规划的库函数进行求解。

