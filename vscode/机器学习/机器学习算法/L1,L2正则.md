参考地址：

https://blog.csdn.net/m0_38045485/article/details/82147817


# L1和L2正则化的来源推导
L1和L2的推导可以从两个角度进行推导：
- 带约束条件的优化求解【拉格朗日乘子法】
- 贝叶斯学派的：最大后验概率

# 1.1 基于约束条件的最优化
对于模型权重系数w的求解是通过最小化下面的目标函数来实现的，也就是求解：
$$\min_{w}\quad J(w;X,y)$$

首先，模型的复杂度可以用VC维来衡量。通常情况下，模型VC维与系数w的个数呈现线性关系，即：
- w数量越多，VC越大，模型越复杂

为了限制模型的复杂度，我们要降低VC，自然的思路就是降低w的数量，即：
- 让w向量中的一些元素为0或者说限制w中非零元素的个数。

我们可以在原来的优化问题上加入一些优化条件：
$$\begin{aligned}
    &\min_{w} \quad J(w;X,y) \\
    &s.t. \quad ||w||_0 \leq C 
\end{aligned}$$

**其中约束条件中的$||w||_0$是指L0范数，表示的是向量w中非零元素的个数，让非零元素的个数小于等于某一个C,就能有效地控制模型中的非零元素的个数。**但是这是一个NP问题，不好解，于是我们需要做一些松弛。

为了达到我们想要的目的--权重向量w中尽可能少的非零项，我们不在严格要求某些权重w为0，而是要求权重w向量中的某些维度的非零参数尽可能的接近于0，尽可能的小，这里我们可以使用L1和L2范数来替代L0范数，即：
$$\begin{aligned}
    &\min_{w} \quad J(w;X,y)\quad 或者 \quad \min_{w}\quad J(w;X,y) \\
    &s.t. \quad ||w||_1 \leq C \qquad\qquad s.t. \quad ||w||^2_2 \leq C
\end{aligned}$$

**注意这里相当于在原有的损失函数的基础上，加上了不等式约束.**

接下来，我们利用拉格朗日乘子法进行求解：
$$\begin{aligned}
    &L(w,\alpha) = J(w;X,y) + \alpha(||w||_1 -C) 或 \\
    &L(w, \alpha) = J(w;X,y) + \alpha(||w||_2^2 -C)
\end{aligned}$$

这里的$\alpha$就是拉格朗日系数， $\alpha >0$，我们假设$\alpha$的最优解为$\alpha^*$，对拉格朗日函数求最小化等价于：
$$\begin{aligned}
    &\min_{w} \quad J(w;X,y) + \alpha^*||w||_1 或 \\
    &\min_{w} \quad J(w;X,y) + \alpha^*||w||_2^2
\end{aligned}$$

所以，这里我们可以得到对L1和L2正则化的第一种理解：
- L1正则化 $\leftrightarrows$ **在原优化目标函数中增加约束条件$||w||_1 \leq C$**
- L2正则化 $\leftrightarrows$ **在原优化目标函数中增加约束条件$||w||_2^2 \leq C$**



# 1.2 基于最大后验概率

后验概率和似然函数，先验概率的关系如下：
$$p(w|x) = \frac{p(x|w)*p(w)}{p(x)}$$

其中$p(w|x)$是后验概率， $p(x|w)$是似然函数， $p(w)$是先验概率。（这里的x其实就是label y）

在机器学习中，我们在知道了数据x以后，需要对参数w进行建模，那么后验概率的表达式如下：
$$MAP = logP(x|w)p(w) = logP(x|w) + logP(w)$$

可以看出来后验概率函数为在似然函数的基础上增加了logP(w)，P(w)的意义是对权重系数w的概率分布的先验假设，在收集到训练样本x后，则可根据w在x下的后验概率对w进行修正，从而做出对w更好地估计。若假设w的先验分布为0均值的高斯分布，即：
$$w ~ N(0, \sigma^2)$$

则有：
$$\begin{aligned}
    log P(w) &= log\prod_{j}P(w_j) \\
             &= log \prod_{j}[\frac{1}{\sqrt{2\pi}} e^{-\frac{(w_j)^2}{2\sigma^2}}] \\
             &= -\frac{1}{2\sigma^2} \sum_{j}w_j^2 +C
\end{aligned} $$

前面我们已经说了，后验概率等于在似然函数的基础上加上了logP(w)，**而上面我们推导出了在高斯分布下logP(w)的效果就等价于在代价函数中增加L2正则项。**

若假设w服从均值为0，参数为$a$的拉普拉斯分布，即：
$$p(w_j) = \frac{1}{\sqrt{2a}}e^{\frac{|w_j|}{a}}$$

那么就有：
$$\begin{aligned}
    logP(w) &= log \prod_{j}P(w_j) \\
            &= log \prod_{j}[\frac{1}{\sqrt{2a}}e^{-\frac{w_j}{a}}] \\
            &= \frac{1}{2a} \sum_{j}|w_j| + C
\end{aligned}$$

所以，我们可以看到，在拉普拉斯分布下logP(w)的效果等价于在代价函数中增加L1正则项。

总结：
- L1正则化可以通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率推导出来
- L2正则化可以通过假设权重w的先验分布为高斯分布，由最大后验概率推导出来





