Bagging和Boosting都是将已有的分类或回归算法通过一定的方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。

## Bagging
Bagging算法即套袋法，在了解其之前，我们先了解下BootStraping，即自助法：是一种有放回的抽样方法（可能抽到重复的样本）。

Bagging的具体算法过程如下：
- A) 从原始样本集中抽取训练集。每轮从原始样本集中使用BootStraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
- B) 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注意：这里并没有具体的分类算法或回归算法，我们可以根据具体问题采用不同的分类或回归方法，比如决策树、感知器等）
- C) 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）


## Boosting
其主要思想是将弱分类器组装成一个强分类器。在PAC(概率近似正确)的学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：
- 1） 在每一轮如何改变训练数据的权值或者概率分布？
  - 通过提高那些在前一轮被弱分类器分错的样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

- 2) 通过什么方式来组合弱分类器
  - 可以通过加法模型将弱分类器进行线性组合，比如AdaBooost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树（Boosting tree）通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终的模型。

## Bagging和Boosting的区别
1） 样本选择上：
- Bagging：训练集是在原始集中有放回的选取的，从原始集中选出的各轮训练集之间是相互独立的；
- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。


2）样例权重：
- Bagging：使用均匀采样，每个样例的权重相等
- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大

3）预测函数：
- Bagging：所有预测函数的权重相等
- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重

4）并行计算：
- Bagging：各个预测函数可以并行生成
- Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

## 总结
这两种方法都是把若干个分类器整合成一个分类器的方法，只是整合的方式不一样，最终得到的结果也不一样，将不同的分类算法套入到此类算法框架中，一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

下面是将决策树与这些算法框架进行结合所得到的新的算法：
- 1）Bagging + 决策树 = 随机森林
- 2）AdaBoost + 决策树 = 提升树
- 3）Gradient Boosting + 决策树 = GBDT 


